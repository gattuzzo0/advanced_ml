{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './asl_data'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print how many classes do we have on our collection (To use it on CNN output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  6  2 12 15  8 21 17  9 19 16 18 20 22 23  1 11 10 14  4  0  5  7 13]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"label\"].unique())\n",
    "print(len(train_df[\"label\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import train and validation datasets from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train dataset is:  (27455, 784)\n",
      "Shape of x_val dataset is:  (7172, 784)\n",
      "Shape of y_train dataset is:  (27455,)\n",
      "Shape of y_train dataset is:  (7172,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "\n",
    "#deleting output label ('label')(Column to predict)\n",
    "train_df = train_df.drop('label', axis=1)\n",
    "valid_df = valid_df.drop('label', axis=1)\n",
    "\n",
    "## Convertion to float shouldn't ocurr here but latter into the Normalisation phase\n",
    "#x_train = train_df.values.astype(np.float32)\n",
    "#x_val = valid_df.values.astype(np.float32)\n",
    "x_train = train_df.values\n",
    "x_val = valid_df.values\n",
    "\n",
    "#Let's check the shape of the dataframes (train and validation loaded from csv files)\n",
    "print(\"Shape of x_train dataset is: \", x_train. shape)\n",
    "print(\"Shape of x_val dataset is: \", x_val. shape)\n",
    "\n",
    "print(\"Shape of y_train dataset is: \", y_train. shape)\n",
    "print(\"Shape of y_train dataset is: \", y_val. shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split \"validation\"  --> 'validation'  and 'test' datasets (to avoid memory leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_val dataset is:  (5020, 784)\n",
      "Shape of x_test dataset is:  (2152, 784)\n",
      "Shape of y_val dataset is:  (5020,)\n",
      "Shape of y_test dataset is:  (2152,)\n"
     ]
    }
   ],
   "source": [
    "#def split_val_test(x, y, pct=0.3, shuffle=True):\n",
    "#    '''\n",
    "#    Create a function that will allow you to split the previously loaded validation set\n",
    "#    into valition and test.\n",
    "#    '''\n",
    "#    x_val, y_val, x_test, y_test = train_test_split(x, y, pct, random_state=1)    \n",
    "#    return x_val, y_val, x_test, y_test\n",
    "#    pass\n",
    "#\n",
    "\n",
    "#Instead of using a custom function, lets use ready to use splitting function from scikit-learn package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val , y_val, test_size=0.3, random_state=1)   \n",
    "\n",
    "print(\"Shape of x_val dataset is: \", x_val. shape)\n",
    "print(\"Shape of x_test dataset is: \", x_test. shape)\n",
    "\n",
    "print(\"Shape of y_val dataset is: \", y_val. shape)\n",
    "print(\"Shape of y_test dataset is: \", y_test. shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "At this point, we can infer:\n",
    "\n",
    "        27455 images belong to train dataset\n",
    "        5020 images belong to validation dataset\n",
    "        2152 images belong to test dataset\n",
    "\n",
    "        784 pixels -> 1 image (1 row on csv)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Possible letters to predict:  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n"
     ]
    }
   ],
   "source": [
    "### Print all possible characters of the alphabet\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "print(alphabet)\n",
    "\n",
    "#Removing 2 undesired letters to predict\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "\n",
    "#Print possible letters to predict\n",
    "print('Possible letters to predict: ', alphabet)\n",
    "#print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Normalise\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how 1 image looks before normalization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how non normalized or converted array looks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([107, 118, 127, 134, 139, 143, 146, 150, 153, 156, 158, 160, 163,\n",
       "       165, 159, 166, 168, 170, 170, 171, 171, 171, 172, 171, 171, 170,\n",
       "       170, 169, 111, 121, 129, 135, 141, 144, 148, 151, 154, 157, 160,\n",
       "       163, 164, 170, 119, 152, 171, 171, 170, 171, 172, 172, 172, 172,\n",
       "       172, 171, 171, 170, 113, 123, 131, 137, 142, 145, 150, 152, 155,\n",
       "       158, 161, 163, 164, 172, 105, 142, 170, 171, 171, 171, 172, 172,\n",
       "       173, 173, 172, 171, 171, 171, 116, 125, 133, 139, 143, 146, 151,\n",
       "       153, 156, 159, 162, 163, 167, 167,  95, 144, 171, 172, 172, 172,\n",
       "       172, 172, 173, 173, 173, 172, 172, 171, 117, 126, 134, 140, 145,\n",
       "       149, 153, 156, 158, 161, 163, 164, 175, 156,  87, 154, 172, 173,\n",
       "       173, 173, 173, 173, 174, 174, 174, 173, 172, 172, 119, 128, 136,\n",
       "       142, 146, 150, 153, 156, 159, 163, 165, 164, 184, 148,  89, 164,\n",
       "       172, 174, 174, 174, 174, 175, 175, 174, 175, 174, 173, 173, 122,\n",
       "       130, 138, 143, 147, 150, 154, 158, 162, 165, 166, 172, 181, 128,\n",
       "        94, 170, 173, 175, 174, 175, 176, 177, 177, 177, 177, 175, 175,\n",
       "       174, 122, 132, 139, 145, 149, 152, 156, 160, 163, 165, 166, 181,\n",
       "       172, 103, 113, 175, 176, 178, 178, 179, 179, 179, 179, 178, 179,\n",
       "       177, 175, 174, 125, 134, 141, 147, 150, 153, 157, 161, 164, 167,\n",
       "       168, 184, 179, 116, 126, 165, 176, 179, 180, 180, 181, 180, 180,\n",
       "       180, 179, 178, 177, 176, 128, 135, 142, 148, 152, 154, 158, 162,\n",
       "       165, 168, 170, 187, 180, 156, 161, 124, 143, 179, 178, 178, 181,\n",
       "       182, 181, 180, 181, 180, 179, 179, 129, 136, 144, 150, 153, 155,\n",
       "       159, 163, 166, 169, 172, 187, 184, 153, 102, 117, 110, 175, 169,\n",
       "       154, 182, 183, 183, 182, 182, 181, 181, 179, 131, 138, 145, 150,\n",
       "       155, 157, 161, 165, 168, 174, 190, 189, 175, 146,  94,  97, 113,\n",
       "       151, 158, 129, 184, 184, 184, 184, 183, 183, 182, 180, 131, 139,\n",
       "       146, 151, 155, 159, 163, 167, 175, 182, 179, 171, 159, 114, 102,\n",
       "        89, 121, 136, 136,  96, 172, 186, 186, 185, 185, 184, 182, 181,\n",
       "       131, 140, 147, 154, 157, 160, 164, 179, 186, 191, 187, 180, 157,\n",
       "       100,  88,  84, 108, 111, 126,  90, 120, 186, 187, 187, 186, 185,\n",
       "       184, 182, 133, 141, 149, 155, 158, 160, 174, 201, 189, 165, 151,\n",
       "       143, 146, 120,  87,  78,  87,  76, 108,  98,  96, 181, 188, 187,\n",
       "       186, 186, 185, 183, 133, 141, 150, 156, 160, 161, 179, 197, 174,\n",
       "       135,  99,  72,  95, 134,  97,  72,  74,  68, 116, 105, 108, 187,\n",
       "       189, 187, 187, 186, 186, 185, 134, 143, 151, 156, 161, 163, 179,\n",
       "       194, 156, 110,  74,  42,  52, 139,  94,  67,  75,  75, 118, 106,\n",
       "       129, 189, 191, 190, 188, 188, 187, 186, 135, 144, 152, 158, 163,\n",
       "       163, 177, 193, 161, 122,  84,  43,  71, 134,  81,  57,  71,  88,\n",
       "       112,  98, 157, 193, 193, 192, 190, 190, 189, 188, 136, 144, 152,\n",
       "       158, 162, 163, 176, 192, 164, 128,  98,  62,  60, 100,  71,  76,\n",
       "        96, 101, 105,  95, 174, 195, 194, 194, 194, 193, 191, 190, 137,\n",
       "       145, 152, 159, 164, 165, 178, 191, 164, 135, 113,  82,  59,  87,\n",
       "        98, 111, 120, 108,  97, 108, 190, 196, 195, 195, 194, 193, 193,\n",
       "       192, 139, 146, 154, 160, 164, 165, 175, 186, 163, 139, 112,  85,\n",
       "        67, 102, 126, 133, 126, 105, 104, 176, 197, 198, 197, 196, 195,\n",
       "       195, 194, 193, 138, 147, 155, 161, 165, 167, 172, 186, 163, 137,\n",
       "       107,  87,  76, 106, 122, 125, 117,  96, 156, 199, 199, 200, 198,\n",
       "       196, 196, 195, 195, 194, 139, 148, 156, 163, 166, 168, 172, 180,\n",
       "       158, 131, 108,  99,  86, 108, 118, 116, 103, 107, 191, 202, 201,\n",
       "       200, 200, 200, 199, 197, 198, 196, 140, 149, 157, 164, 168, 167,\n",
       "       177, 178, 155, 131, 118, 105,  87, 100, 106, 100,  96, 164, 202,\n",
       "       202, 202, 202, 202, 201, 200, 199, 199, 198, 140, 150, 157, 165,\n",
       "       167, 170, 181, 175, 152, 130, 115,  98,  82,  85,  90,  99, 165,\n",
       "       202, 203, 204, 203, 203, 202, 202, 201, 201, 200, 200, 142, 150,\n",
       "       159, 165, 170, 191, 173, 157, 144, 119,  97,  84,  79,  79,  91,\n",
       "       172, 202, 203, 203, 205, 204, 204, 204, 203, 202, 202, 201, 200,\n",
       "       142, 151, 160, 165, 188, 190, 187, 150, 119, 109,  85,  79,  79,\n",
       "        78, 137, 203, 205, 206, 206, 207, 207, 206, 206, 204, 205, 204,\n",
       "       203, 202, 142, 151, 160, 172, 196, 188, 188, 190, 135,  96,  86,\n",
       "        77,  77,  79, 176, 205, 207, 207, 207, 207, 207, 207, 206, 206,\n",
       "       206, 204, 203, 202], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing first sample as INT array\n",
    "print(\"This is how non normalized or converted array looks\")\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"MinMax\" normalization function from sklearn package also...\n",
    "\n",
    "The intention here is to scale all the values of the train dataset, getting min and max values, converting this to 0 and 1, and converting everything on the middle to a float number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how NORMALIZED array looks like:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.41960784, 0.4627451 , 0.49803922, 0.5254902 , 0.54509804,\n",
       "       0.56078431, 0.57254902, 0.58823529, 0.6       , 0.61176471,\n",
       "       0.61960784, 0.62745098, 0.63921569, 0.64705882, 0.62352941,\n",
       "       0.62605042, 0.625     , 0.62555066, 0.61538462, 0.67058824,\n",
       "       0.67058824, 0.67058824, 0.6745098 , 0.67058824, 0.67058824,\n",
       "       0.66666667, 0.66666667, 0.6627451 , 0.43529412, 0.4745098 ,\n",
       "       0.50588235, 0.52941176, 0.55294118, 0.56470588, 0.58039216,\n",
       "       0.59215686, 0.60392157, 0.61568627, 0.62745098, 0.63921569,\n",
       "       0.64313725, 0.66666667, 0.46666667, 0.56540084, 0.63478261,\n",
       "       0.62666667, 0.61187215, 0.65853659, 0.6745098 , 0.6745098 ,\n",
       "       0.6745098 , 0.6745098 , 0.6745098 , 0.67058824, 0.67058824,\n",
       "       0.66666667, 0.44313725, 0.48235294, 0.51372549, 0.5372549 ,\n",
       "       0.55686275, 0.56862745, 0.58823529, 0.59607843, 0.60784314,\n",
       "       0.61960784, 0.63137255, 0.63921569, 0.64313725, 0.6745098 ,\n",
       "       0.41176471, 0.52118644, 0.65306122, 0.64556962, 0.62331839,\n",
       "       0.63636364, 0.6745098 , 0.6745098 , 0.67843137, 0.67843137,\n",
       "       0.6745098 , 0.67058824, 0.67058824, 0.67058824, 0.45490196,\n",
       "       0.49019608, 0.52156863, 0.54509804, 0.56078431, 0.57254902,\n",
       "       0.59215686, 0.6       , 0.61176471, 0.62352941, 0.63529412,\n",
       "       0.63921569, 0.65490196, 0.65490196, 0.37254902, 0.556     ,\n",
       "       0.65991903, 0.65271967, 0.65560166, 0.66396761, 0.6745098 ,\n",
       "       0.6745098 , 0.67843137, 0.67843137, 0.67843137, 0.6745098 ,\n",
       "       0.6745098 , 0.67058824, 0.45882353, 0.49411765, 0.5254902 ,\n",
       "       0.54901961, 0.56862745, 0.58431373, 0.6       , 0.61176471,\n",
       "       0.61960784, 0.63137255, 0.63921569, 0.64313725, 0.68627451,\n",
       "       0.61176471, 0.34117647, 0.6023622 , 0.6745098 , 0.67330677,\n",
       "       0.66801619, 0.67843137, 0.67843137, 0.67843137, 0.68235294,\n",
       "       0.68235294, 0.68235294, 0.67843137, 0.6745098 , 0.6745098 ,\n",
       "       0.46666667, 0.50196078, 0.53333333, 0.55686275, 0.57254902,\n",
       "       0.58823529, 0.6       , 0.61176471, 0.61133603, 0.63921569,\n",
       "       0.64705882, 0.64313725, 0.72156863, 0.58039216, 0.34901961,\n",
       "       0.64313725, 0.6745098 , 0.68235294, 0.68235294, 0.68235294,\n",
       "       0.67073171, 0.68627451, 0.68627451, 0.68235294, 0.68627451,\n",
       "       0.68235294, 0.67843137, 0.67843137, 0.47843137, 0.50980392,\n",
       "       0.54117647, 0.56078431, 0.57647059, 0.58823529, 0.60392157,\n",
       "       0.61960784, 0.61728395, 0.64285714, 0.65098039, 0.6745098 ,\n",
       "       0.70980392, 0.50196078, 0.36862745, 0.66666667, 0.67843137,\n",
       "       0.68627451, 0.68235294, 0.68627451, 0.69019608, 0.69411765,\n",
       "       0.69411765, 0.69411765, 0.69411765, 0.68627451, 0.68627451,\n",
       "       0.68235294, 0.47843137, 0.51764706, 0.54509804, 0.56862745,\n",
       "       0.58431373, 0.59607843, 0.61176471, 0.62745098, 0.63052209,\n",
       "       0.64705882, 0.65098039, 0.70980392, 0.6745098 , 0.40392157,\n",
       "       0.44313725, 0.68627451, 0.69019608, 0.69803922, 0.69803922,\n",
       "       0.70196078, 0.70196078, 0.70196078, 0.70196078, 0.69803922,\n",
       "       0.70196078, 0.69411765, 0.68627451, 0.68235294, 0.49019608,\n",
       "       0.5254902 , 0.55294118, 0.57647059, 0.58823529, 0.6       ,\n",
       "       0.61568627, 0.63137255, 0.64031621, 0.65490196, 0.65882353,\n",
       "       0.72156863, 0.70196078, 0.45490196, 0.49411765, 0.64705882,\n",
       "       0.69019608, 0.70196078, 0.70588235, 0.70588235, 0.70980392,\n",
       "       0.70588235, 0.70588235, 0.70588235, 0.70196078, 0.69803922,\n",
       "       0.69411765, 0.69019608, 0.50196078, 0.52941176, 0.55686275,\n",
       "       0.58039216, 0.59607843, 0.60392157, 0.61960784, 0.63529412,\n",
       "       0.64705882, 0.65882353, 0.66666667, 0.73333333, 0.70588235,\n",
       "       0.61176471, 0.63137255, 0.48627451, 0.56078431, 0.70196078,\n",
       "       0.69803922, 0.69803922, 0.70980392, 0.71372549, 0.70980392,\n",
       "       0.70588235, 0.70980392, 0.70588235, 0.70196078, 0.70196078,\n",
       "       0.50588235, 0.53333333, 0.56470588, 0.58823529, 0.6       ,\n",
       "       0.60784314, 0.62352941, 0.63921569, 0.65098039, 0.6627451 ,\n",
       "       0.6745098 , 0.73333333, 0.72156863, 0.6       , 0.4       ,\n",
       "       0.45882353, 0.43137255, 0.68627451, 0.6627451 , 0.60392157,\n",
       "       0.71372549, 0.71764706, 0.71764706, 0.71372549, 0.71372549,\n",
       "       0.70980392, 0.70980392, 0.70196078, 0.51372549, 0.54117647,\n",
       "       0.56862745, 0.58823529, 0.60784314, 0.61568627, 0.63137255,\n",
       "       0.64705882, 0.65882353, 0.68235294, 0.74509804, 0.74117647,\n",
       "       0.68627451, 0.57254902, 0.36862745, 0.38039216, 0.44313725,\n",
       "       0.59215686, 0.61960784, 0.50588235, 0.72156863, 0.72156863,\n",
       "       0.72156863, 0.72156863, 0.71764706, 0.71764706, 0.71372549,\n",
       "       0.70588235, 0.51372549, 0.54509804, 0.57254902, 0.59215686,\n",
       "       0.60784314, 0.62352941, 0.63921569, 0.65490196, 0.68627451,\n",
       "       0.71372549, 0.70196078, 0.67058824, 0.62352941, 0.44705882,\n",
       "       0.4       , 0.34901961, 0.4745098 , 0.53333333, 0.53333333,\n",
       "       0.37647059, 0.6745098 , 0.72941176, 0.72941176, 0.7254902 ,\n",
       "       0.7254902 , 0.72156863, 0.71372549, 0.70980392, 0.51372549,\n",
       "       0.54901961, 0.57647059, 0.60392157, 0.61568627, 0.62745098,\n",
       "       0.64313725, 0.7007874 , 0.72941176, 0.74901961, 0.73333333,\n",
       "       0.70588235, 0.61568627, 0.39215686, 0.34509804, 0.32941176,\n",
       "       0.42352941, 0.43529412, 0.49411765, 0.35294118, 0.47058824,\n",
       "       0.72941176, 0.73333333, 0.73333333, 0.72941176, 0.7254902 ,\n",
       "       0.72156863, 0.71372549, 0.52156863, 0.55294118, 0.58431373,\n",
       "       0.60784314, 0.61960784, 0.62745098, 0.68235294, 0.78823529,\n",
       "       0.74117647, 0.64705882, 0.59215686, 0.56078431, 0.57254902,\n",
       "       0.47058824, 0.34117647, 0.30588235, 0.34117647, 0.29803922,\n",
       "       0.42352941, 0.38431373, 0.37647059, 0.70980392, 0.7372549 ,\n",
       "       0.73333333, 0.72941176, 0.72941176, 0.7254902 , 0.71764706,\n",
       "       0.52156863, 0.55294118, 0.58823529, 0.61176471, 0.62745098,\n",
       "       0.63137255, 0.70196078, 0.7689243 , 0.68235294, 0.52941176,\n",
       "       0.38823529, 0.28235294, 0.37254902, 0.5254902 , 0.38039216,\n",
       "       0.28235294, 0.29019608, 0.26666667, 0.45490196, 0.41176471,\n",
       "       0.42352941, 0.73333333, 0.74117647, 0.73333333, 0.73333333,\n",
       "       0.72941176, 0.72941176, 0.7254902 , 0.5254902 , 0.56078431,\n",
       "       0.59215686, 0.61176471, 0.63137255, 0.63921569, 0.70196078,\n",
       "       0.75502008, 0.61176471, 0.43137255, 0.29019608, 0.16470588,\n",
       "       0.20392157, 0.54509804, 0.36862745, 0.2627451 , 0.29411765,\n",
       "       0.29411765, 0.4627451 , 0.41568627, 0.50588235, 0.74117647,\n",
       "       0.74901961, 0.74509804, 0.7372549 , 0.7372549 , 0.73333333,\n",
       "       0.72941176, 0.52941176, 0.56470588, 0.59607843, 0.61960784,\n",
       "       0.63921569, 0.63921569, 0.69411765, 0.75686275, 0.63137255,\n",
       "       0.47843137, 0.32941176, 0.16862745, 0.27843137, 0.5254902 ,\n",
       "       0.31764706, 0.22352941, 0.27843137, 0.34509804, 0.43921569,\n",
       "       0.38431373, 0.61568627, 0.75686275, 0.75686275, 0.75294118,\n",
       "       0.74509804, 0.74509804, 0.74117647, 0.7372549 , 0.53333333,\n",
       "       0.56470588, 0.59607843, 0.61960784, 0.63529412, 0.63921569,\n",
       "       0.69019608, 0.75294118, 0.63888889, 0.50196078, 0.38431373,\n",
       "       0.24313725, 0.23529412, 0.39215686, 0.27843137, 0.29803922,\n",
       "       0.37647059, 0.39607843, 0.41176471, 0.37254902, 0.68235294,\n",
       "       0.76470588, 0.76078431, 0.76078431, 0.76078431, 0.75686275,\n",
       "       0.74901961, 0.74509804, 0.5372549 , 0.56862745, 0.59607843,\n",
       "       0.62352941, 0.64313725, 0.64705882, 0.69803922, 0.74901961,\n",
       "       0.64313725, 0.52941176, 0.44313725, 0.32156863, 0.23137255,\n",
       "       0.34117647, 0.38431373, 0.43529412, 0.47058824, 0.42352941,\n",
       "       0.38039216, 0.42352941, 0.74509804, 0.76862745, 0.76470588,\n",
       "       0.76470588, 0.76078431, 0.75686275, 0.75686275, 0.75294118,\n",
       "       0.54509804, 0.57254902, 0.60392157, 0.62745098, 0.64313725,\n",
       "       0.64705882, 0.68627451, 0.72941176, 0.63921569, 0.54509804,\n",
       "       0.43921569, 0.33333333, 0.2627451 , 0.4       , 0.49411765,\n",
       "       0.52156863, 0.49411765, 0.41176471, 0.40784314, 0.69019608,\n",
       "       0.77254902, 0.77647059, 0.77254902, 0.76862745, 0.76470588,\n",
       "       0.76470588, 0.76078431, 0.75686275, 0.54117647, 0.57647059,\n",
       "       0.60784314, 0.63137255, 0.64705882, 0.65490196, 0.6745098 ,\n",
       "       0.72941176, 0.63921569, 0.5372549 , 0.41960784, 0.34117647,\n",
       "       0.29803922, 0.41568627, 0.47843137, 0.49019608, 0.45882353,\n",
       "       0.37647059, 0.61176471, 0.78039216, 0.78039216, 0.78431373,\n",
       "       0.77647059, 0.76862745, 0.76862745, 0.76470588, 0.76470588,\n",
       "       0.76078431, 0.54509804, 0.58039216, 0.61176471, 0.63921569,\n",
       "       0.65098039, 0.65882353, 0.6745098 , 0.70588235, 0.61960784,\n",
       "       0.51372549, 0.42352941, 0.38823529, 0.3372549 , 0.42352941,\n",
       "       0.4627451 , 0.45490196, 0.40392157, 0.41960784, 0.74901961,\n",
       "       0.79215686, 0.78823529, 0.78431373, 0.78431373, 0.78431373,\n",
       "       0.78039216, 0.77254902, 0.77647059, 0.76862745, 0.54901961,\n",
       "       0.58431373, 0.61568627, 0.64313725, 0.65882353, 0.65490196,\n",
       "       0.69411765, 0.69803922, 0.60784314, 0.51372549, 0.4627451 ,\n",
       "       0.41176471, 0.34117647, 0.39215686, 0.41568627, 0.39215686,\n",
       "       0.37647059, 0.64313725, 0.79215686, 0.79215686, 0.79215686,\n",
       "       0.79215686, 0.79215686, 0.78823529, 0.78431373, 0.78039216,\n",
       "       0.78039216, 0.77647059, 0.54901961, 0.58823529, 0.61568627,\n",
       "       0.64705882, 0.65490196, 0.66666667, 0.70980392, 0.68627451,\n",
       "       0.59607843, 0.50980392, 0.45098039, 0.38431373, 0.32156863,\n",
       "       0.33333333, 0.35294118, 0.38823529, 0.64705882, 0.79215686,\n",
       "       0.79607843, 0.8       , 0.79607843, 0.79607843, 0.79215686,\n",
       "       0.79215686, 0.78823529, 0.78823529, 0.78431373, 0.78431373,\n",
       "       0.55686275, 0.58823529, 0.62352941, 0.64705882, 0.66666667,\n",
       "       0.74901961, 0.67843137, 0.61568627, 0.56470588, 0.46666667,\n",
       "       0.38039216, 0.32941176, 0.30980392, 0.30980392, 0.35686275,\n",
       "       0.6745098 , 0.79215686, 0.79607843, 0.79607843, 0.80392157,\n",
       "       0.8       , 0.8       , 0.8       , 0.79607843, 0.79215686,\n",
       "       0.79215686, 0.78823529, 0.78431373, 0.55686275, 0.59215686,\n",
       "       0.62745098, 0.64705882, 0.7372549 , 0.74509804, 0.73333333,\n",
       "       0.58823529, 0.46666667, 0.42745098, 0.33333333, 0.30980392,\n",
       "       0.30980392, 0.30588235, 0.5372549 , 0.79607843, 0.80392157,\n",
       "       0.80784314, 0.80784314, 0.81176471, 0.81176471, 0.80784314,\n",
       "       0.80784314, 0.8       , 0.80392157, 0.8       , 0.79607843,\n",
       "       0.79215686, 0.55686275, 0.59215686, 0.62745098, 0.6745098 ,\n",
       "       0.76862745, 0.7372549 , 0.7372549 , 0.74509804, 0.52941176,\n",
       "       0.37647059, 0.3372549 , 0.30196078, 0.30196078, 0.30980392,\n",
       "       0.69019608, 0.80392157, 0.81176471, 0.81176471, 0.81176471,\n",
       "       0.81176471, 0.81176471, 0.81176471, 0.80784314, 0.80784314,\n",
       "       0.80784314, 0.8       , 0.79607843, 0.79215686])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "x_train = minmax_scale(x_train)\n",
    "#x_train_normalized.shape\n",
    "\n",
    "print(\"This is how NORMALIZED array looks like:\")\n",
    "x_train[0]  #Prints array example ( first row of csv dataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how a random selected array from dataset, converted to 28*28 image looks like\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_number(image):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#Generates a random integer\n",
    "rnd_idx = np.random.randint(len(y_test))\n",
    "\n",
    "#Reshape array form 784*1 1D np.array   -->   28*28 2D np.array\n",
    "# to make it possible to be plot as greyscale image\n",
    "print(\"This is how a random selected array from dataset, converted to 28*28 image looks like\")\n",
    "plot_number(x_train[rnd_idx].reshape(28,28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones para nuestro modelo\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    '''\n",
    "    x  #muestras, 784\n",
    "    y #muestras, 1\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class np_tensor(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 0])\n",
    "b = a.view(np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.np_tensor"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np_tensor([ True,  True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a is b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Init parameters utilizando Kaiming He\n",
    "        '''\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
    "    def __call__(self, X): # esta el foward de la clase lineal\n",
    "        Z = self.W @ X + self.b\n",
    "        return Z\n",
    "    def backward(self, X, Z):\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        self.W.grad = Z.grad @ X.T\n",
    "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def backward(self, Z, A):\n",
    "        Z.grad = A.grad.copy()\n",
    "        Z.grad[Z <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_layers():\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        layers - lista que contiene objetos de tipo Linear, ReLU\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.x = None\n",
    "        self.outputs = {}\n",
    "    def __call__(self, X):\n",
    "        self.x = X \n",
    "        self.outputs['l0'] = self.x\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            self.x = layer(self.x)\n",
    "            self.outputs['l'+str(i)]=self.x\n",
    "        return self.x\n",
    "    def backward(self):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])\n",
    "    def update(self, learning_rate = 1e-3):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLU): continue\n",
    "            layer.W = layer.W - learning_rate * layer.W.grad\n",
    "            layer.b = layer.b - learning_rate * layer.b.grad\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.__call__(X))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    batch_size = x.shape[1]\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
    "    preds = probs.copy()\n",
    "    # Costo\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    # Calcular gradientes\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1 #dl/dx\n",
    "    x.grad = probs.copy()\n",
    "    \n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size=128, learning_rate = 1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            scores = model(x.T.view(np_tensor))\n",
    "            _, cost = softmaxXEntropy(scores, y)\n",
    "            model.backward()\n",
    "            model.update(learning_rate)\n",
    "        print(f'costo: {cost}, accuracy: {accuracy(x_val, y_val, mb_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y)):\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
    "        total += pred.shape[1]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to change the output classes according to possible predictions (in this case 24 possible classes found on train_df)\n",
    "\n",
    "#1st hidden layer\n",
    "#Linear(784, 200) --> 784 inputs / 200 outputs\n",
    "#ReLU() Activation type\n",
    "\n",
    "#2nd hidden layer --> 200 inputs / 200 outputs. The inputs are the outputs of the first hidden layer\n",
    "#Linear(200, 200)\n",
    "#ReLU() Activation type\n",
    "\n",
    "#Classification layer  --> 200 inputs / 24 classes to predict\n",
    "#Linear(200,24)\n",
    "\n",
    "model = Sequential_layers([Linear(784, 32), ReLU(), \n",
    "                            Linear(32, 64), ReLU(), \n",
    "                            Linear(64, 128), ReLU(), \n",
    "                            Linear(128,24)])\n",
    "\n",
    "#mini batch size is mainly limited by the hardware we use for the training (OOM error will ocurr when exceeding)\n",
    "# The smaller the batch size, the slower it takes to train\n",
    "# Not necesarily the bigger the value, the better the accuracy\n",
    "# Normally we use multiple of bytes (8, 32, 64, 128, 256, etc...)\n",
    "mb_size = 256\n",
    "\n",
    "# The smaller the learning rate is, the slower it takes to train\n",
    "# Using a BIG learning rate is prompt to overfit \n",
    "learning_rate = 0.0001\n",
    "\n",
    "#50 epochs were needed to reach a decent level of accuracy using this fully connected network\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo: 2.9840483691457678, accuracy: 0.0944223107569721\n",
      "costo: 2.4836549117598588, accuracy: 0.22868525896414343\n",
      "costo: 2.3278848872193048, accuracy: 0.29262948207171313\n",
      "costo: 2.172791319225743, accuracy: 0.4362549800796813\n",
      "costo: 1.870298641495941, accuracy: 0.41733067729083667\n",
      "costo: 1.7591860942209392, accuracy: 0.42868525896414345\n",
      "costo: 1.76812274765895, accuracy: 0.49382470119521915\n",
      "costo: 1.5516329292186133, accuracy: 0.5221115537848605\n",
      "costo: 1.3809796269318153, accuracy: 0.5247011952191235\n",
      "costo: 1.2105837570242144, accuracy: 0.5346613545816733\n",
      "costo: 1.3225730683558135, accuracy: 0.5611553784860558\n",
      "costo: 1.078249548538909, accuracy: 0.5507968127490039\n",
      "costo: 1.7749259019571995, accuracy: 0.4796812749003984\n",
      "costo: 0.9389199140904727, accuracy: 0.5758964143426295\n",
      "costo: 1.1673227589940576, accuracy: 0.5689243027888446\n",
      "costo: 1.1320588731377663, accuracy: 0.5890438247011952\n",
      "costo: 0.8323522146894041, accuracy: 0.598605577689243\n",
      "costo: 1.1478080855141128, accuracy: 0.5924302788844622\n",
      "costo: 1.0465071957053882, accuracy: 0.597808764940239\n",
      "costo: 1.2605431952309134, accuracy: 0.5735059760956175\n",
      "costo: 0.46230217033756044, accuracy: 0.6165338645418327\n",
      "costo: 0.7436319816201407, accuracy: 0.6175298804780877\n",
      "costo: 0.7508264996954557, accuracy: 0.6061752988047808\n",
      "costo: 0.5952347147665179, accuracy: 0.6587649402390439\n",
      "costo: 0.604218944290221, accuracy: 0.6332669322709163\n",
      "costo: 0.46747192708957014, accuracy: 0.6715139442231076\n",
      "costo: 0.54322176492603, accuracy: 0.6693227091633466\n",
      "costo: 0.5709215709001987, accuracy: 0.6806772908366534\n",
      "costo: 0.6328567571364733, accuracy: 0.6422310756972112\n",
      "costo: 0.27529430130989535, accuracy: 0.6745019920318726\n",
      "costo: 0.48399612844258266, accuracy: 0.701394422310757\n",
      "costo: 1.0175562917779, accuracy: 0.6402390438247012\n",
      "costo: 0.37495576724593843, accuracy: 0.6460159362549801\n",
      "costo: 0.2998853471387072, accuracy: 0.7288844621513945\n",
      "costo: 0.2221158263891667, accuracy: 0.7235059760956175\n",
      "costo: 0.16150053931396988, accuracy: 0.7141434262948207\n",
      "costo: 0.24842466493154822, accuracy: 0.7109561752988047\n",
      "costo: 0.21220133518669568, accuracy: 0.7119521912350597\n",
      "costo: 0.20677653172357213, accuracy: 0.7229083665338646\n",
      "costo: 0.14768745719123894, accuracy: 0.7017928286852589\n",
      "costo: 0.34170547063382467, accuracy: 0.6677290836653387\n",
      "costo: 0.2103393659573503, accuracy: 0.7119521912350597\n",
      "costo: 0.1197679679393158, accuracy: 0.7225099601593625\n",
      "costo: 0.16103738182446972, accuracy: 0.7268924302788845\n",
      "costo: 0.10845305350842964, accuracy: 0.7384462151394422\n",
      "costo: 0.08043819577222067, accuracy: 0.7209163346613546\n",
      "costo: 0.08880984535731293, accuracy: 0.7392430278884462\n",
      "costo: 0.08784621070123741, accuracy: 0.747211155378486\n",
      "costo: 0.24083579141653605, accuracy: 0.7229083665338646\n",
      "costo: 0.07795549545327872, accuracy: 0.7420318725099602\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Accuracy reached\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0 %\n"
     ]
    }
   ],
   "source": [
    "print((accuracy(x_test, y_test, mb_size))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor predicho es: p el valor real es:p\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test[idx].reshape(28,28))\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'el valor predicho es: {alphabet[pred]} el valor real es:{alphabet[y_test[idx]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to reach a decent level of accuracy (>70%) NOT ONLY by tunning hyperparameters such as batch size, learning rate, epochs, etc. But we also modify the fully connected network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
