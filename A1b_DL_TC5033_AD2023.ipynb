{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './asl_data'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print how many classes do we have on our collection (To use it on CNN output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  6  2 12 15  8 21 17  9 19 16 18 20 22 23  1 11 10 14  4  0  5  7 13]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"label\"].unique())\n",
    "print(len(train_df[\"label\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import train and validation datasets from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train dataset is:  (27455, 784)\n",
      "Shape of x_val dataset is:  (7172, 784)\n",
      "Shape of y_train dataset is:  (27455,)\n",
      "Shape of y_train dataset is:  (7172,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "\n",
    "#deleting output label ('label')(Column to predict)\n",
    "train_df = train_df.drop('label', axis=1)\n",
    "valid_df = valid_df.drop('label', axis=1)\n",
    "\n",
    "## Convertion to float shouldn't ocurr here but latter into the Normalisation phase\n",
    "#x_train = train_df.values.astype(np.float32)\n",
    "#x_val = valid_df.values.astype(np.float32)\n",
    "x_train = train_df.values\n",
    "x_val = valid_df.values\n",
    "\n",
    "#Let's check the shape of the dataframes (train and validation loaded from csv files)\n",
    "print(\"Shape of x_train dataset is: \", x_train. shape)\n",
    "print(\"Shape of x_val dataset is: \", x_val. shape)\n",
    "\n",
    "print(\"Shape of y_train dataset is: \", y_train. shape)\n",
    "print(\"Shape of y_train dataset is: \", y_val. shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split \"validation\"  --> 'validation'  and 'test' datasets (to avoid memory leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_val dataset is:  (5020, 784)\n",
      "Shape of x_test dataset is:  (2152, 784)\n",
      "Shape of y_val dataset is:  (5020,)\n",
      "Shape of y_test dataset is:  (2152,)\n"
     ]
    }
   ],
   "source": [
    "#def split_val_test(x, y, pct=0.3, shuffle=True):\n",
    "#    '''\n",
    "#    Create a function that will allow you to split the previously loaded validation set\n",
    "#    into valition and test.\n",
    "#    '''\n",
    "#    x_val, y_val, x_test, y_test = train_test_split(x, y, pct, random_state=1)    \n",
    "#    return x_val, y_val, x_test, y_test\n",
    "#    pass\n",
    "#\n",
    "\n",
    "#Instead of using a custom function, lets use ready to use splitting function from scikit-learn package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val , y_val, test_size=0.3, random_state=1)   \n",
    "\n",
    "print(\"Shape of x_val dataset is: \", x_val. shape)\n",
    "print(\"Shape of x_test dataset is: \", x_test. shape)\n",
    "\n",
    "print(\"Shape of y_val dataset is: \", y_val. shape)\n",
    "print(\"Shape of y_test dataset is: \", y_test. shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "At this point, we can infer:\n",
    "\n",
    "        5020 images belong to validation dataset\n",
    "        2152 images belong to test dataset\n",
    "\n",
    "        784 pixels -> 1 image (1 row on csv)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Possible letters to predict:  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n"
     ]
    }
   ],
   "source": [
    "### Print all possible characters of the alphabet\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "print(alphabet)\n",
    "\n",
    "#Removing 2 undesired letters to predict\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "\n",
    "#Print possible letters to predict\n",
    "print('Possible letters to predict: ', alphabet)\n",
    "#print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Normalise\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how 1 image looks before normalization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how non normalized or converted array looks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([107, 118, 127, 134, 139, 143, 146, 150, 153, 156, 158, 160, 163,\n",
       "       165, 159, 166, 168, 170, 170, 171, 171, 171, 172, 171, 171, 170,\n",
       "       170, 169, 111, 121, 129, 135, 141, 144, 148, 151, 154, 157, 160,\n",
       "       163, 164, 170, 119, 152, 171, 171, 170, 171, 172, 172, 172, 172,\n",
       "       172, 171, 171, 170, 113, 123, 131, 137, 142, 145, 150, 152, 155,\n",
       "       158, 161, 163, 164, 172, 105, 142, 170, 171, 171, 171, 172, 172,\n",
       "       173, 173, 172, 171, 171, 171, 116, 125, 133, 139, 143, 146, 151,\n",
       "       153, 156, 159, 162, 163, 167, 167,  95, 144, 171, 172, 172, 172,\n",
       "       172, 172, 173, 173, 173, 172, 172, 171, 117, 126, 134, 140, 145,\n",
       "       149, 153, 156, 158, 161, 163, 164, 175, 156,  87, 154, 172, 173,\n",
       "       173, 173, 173, 173, 174, 174, 174, 173, 172, 172, 119, 128, 136,\n",
       "       142, 146, 150, 153, 156, 159, 163, 165, 164, 184, 148,  89, 164,\n",
       "       172, 174, 174, 174, 174, 175, 175, 174, 175, 174, 173, 173, 122,\n",
       "       130, 138, 143, 147, 150, 154, 158, 162, 165, 166, 172, 181, 128,\n",
       "        94, 170, 173, 175, 174, 175, 176, 177, 177, 177, 177, 175, 175,\n",
       "       174, 122, 132, 139, 145, 149, 152, 156, 160, 163, 165, 166, 181,\n",
       "       172, 103, 113, 175, 176, 178, 178, 179, 179, 179, 179, 178, 179,\n",
       "       177, 175, 174, 125, 134, 141, 147, 150, 153, 157, 161, 164, 167,\n",
       "       168, 184, 179, 116, 126, 165, 176, 179, 180, 180, 181, 180, 180,\n",
       "       180, 179, 178, 177, 176, 128, 135, 142, 148, 152, 154, 158, 162,\n",
       "       165, 168, 170, 187, 180, 156, 161, 124, 143, 179, 178, 178, 181,\n",
       "       182, 181, 180, 181, 180, 179, 179, 129, 136, 144, 150, 153, 155,\n",
       "       159, 163, 166, 169, 172, 187, 184, 153, 102, 117, 110, 175, 169,\n",
       "       154, 182, 183, 183, 182, 182, 181, 181, 179, 131, 138, 145, 150,\n",
       "       155, 157, 161, 165, 168, 174, 190, 189, 175, 146,  94,  97, 113,\n",
       "       151, 158, 129, 184, 184, 184, 184, 183, 183, 182, 180, 131, 139,\n",
       "       146, 151, 155, 159, 163, 167, 175, 182, 179, 171, 159, 114, 102,\n",
       "        89, 121, 136, 136,  96, 172, 186, 186, 185, 185, 184, 182, 181,\n",
       "       131, 140, 147, 154, 157, 160, 164, 179, 186, 191, 187, 180, 157,\n",
       "       100,  88,  84, 108, 111, 126,  90, 120, 186, 187, 187, 186, 185,\n",
       "       184, 182, 133, 141, 149, 155, 158, 160, 174, 201, 189, 165, 151,\n",
       "       143, 146, 120,  87,  78,  87,  76, 108,  98,  96, 181, 188, 187,\n",
       "       186, 186, 185, 183, 133, 141, 150, 156, 160, 161, 179, 197, 174,\n",
       "       135,  99,  72,  95, 134,  97,  72,  74,  68, 116, 105, 108, 187,\n",
       "       189, 187, 187, 186, 186, 185, 134, 143, 151, 156, 161, 163, 179,\n",
       "       194, 156, 110,  74,  42,  52, 139,  94,  67,  75,  75, 118, 106,\n",
       "       129, 189, 191, 190, 188, 188, 187, 186, 135, 144, 152, 158, 163,\n",
       "       163, 177, 193, 161, 122,  84,  43,  71, 134,  81,  57,  71,  88,\n",
       "       112,  98, 157, 193, 193, 192, 190, 190, 189, 188, 136, 144, 152,\n",
       "       158, 162, 163, 176, 192, 164, 128,  98,  62,  60, 100,  71,  76,\n",
       "        96, 101, 105,  95, 174, 195, 194, 194, 194, 193, 191, 190, 137,\n",
       "       145, 152, 159, 164, 165, 178, 191, 164, 135, 113,  82,  59,  87,\n",
       "        98, 111, 120, 108,  97, 108, 190, 196, 195, 195, 194, 193, 193,\n",
       "       192, 139, 146, 154, 160, 164, 165, 175, 186, 163, 139, 112,  85,\n",
       "        67, 102, 126, 133, 126, 105, 104, 176, 197, 198, 197, 196, 195,\n",
       "       195, 194, 193, 138, 147, 155, 161, 165, 167, 172, 186, 163, 137,\n",
       "       107,  87,  76, 106, 122, 125, 117,  96, 156, 199, 199, 200, 198,\n",
       "       196, 196, 195, 195, 194, 139, 148, 156, 163, 166, 168, 172, 180,\n",
       "       158, 131, 108,  99,  86, 108, 118, 116, 103, 107, 191, 202, 201,\n",
       "       200, 200, 200, 199, 197, 198, 196, 140, 149, 157, 164, 168, 167,\n",
       "       177, 178, 155, 131, 118, 105,  87, 100, 106, 100,  96, 164, 202,\n",
       "       202, 202, 202, 202, 201, 200, 199, 199, 198, 140, 150, 157, 165,\n",
       "       167, 170, 181, 175, 152, 130, 115,  98,  82,  85,  90,  99, 165,\n",
       "       202, 203, 204, 203, 203, 202, 202, 201, 201, 200, 200, 142, 150,\n",
       "       159, 165, 170, 191, 173, 157, 144, 119,  97,  84,  79,  79,  91,\n",
       "       172, 202, 203, 203, 205, 204, 204, 204, 203, 202, 202, 201, 200,\n",
       "       142, 151, 160, 165, 188, 190, 187, 150, 119, 109,  85,  79,  79,\n",
       "        78, 137, 203, 205, 206, 206, 207, 207, 206, 206, 204, 205, 204,\n",
       "       203, 202, 142, 151, 160, 172, 196, 188, 188, 190, 135,  96,  86,\n",
       "        77,  77,  79, 176, 205, 207, 207, 207, 207, 207, 207, 206, 206,\n",
       "       206, 204, 203, 202], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing first sample as INT array\n",
    "print(\"This is how non normalized or converted array looks\")\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"MinMax\" normalization function from sklearn package also...\n",
    "\n",
    "The intention here is to scale all the values of the train dataset, getting min and max values, converting this to 0 and 1, and converting everything on the middle to a float number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how NORMALIZED array looks like:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.41960784, 0.4627451 , 0.49803922, 0.5254902 , 0.54509804,\n",
       "       0.56078431, 0.57254902, 0.58823529, 0.6       , 0.61176471,\n",
       "       0.61960784, 0.62745098, 0.63921569, 0.64705882, 0.62352941,\n",
       "       0.62605042, 0.625     , 0.62555066, 0.61538462, 0.67058824,\n",
       "       0.67058824, 0.67058824, 0.6745098 , 0.67058824, 0.67058824,\n",
       "       0.66666667, 0.66666667, 0.6627451 , 0.43529412, 0.4745098 ,\n",
       "       0.50588235, 0.52941176, 0.55294118, 0.56470588, 0.58039216,\n",
       "       0.59215686, 0.60392157, 0.61568627, 0.62745098, 0.63921569,\n",
       "       0.64313725, 0.66666667, 0.46666667, 0.56540084, 0.63478261,\n",
       "       0.62666667, 0.61187215, 0.65853659, 0.6745098 , 0.6745098 ,\n",
       "       0.6745098 , 0.6745098 , 0.6745098 , 0.67058824, 0.67058824,\n",
       "       0.66666667, 0.44313725, 0.48235294, 0.51372549, 0.5372549 ,\n",
       "       0.55686275, 0.56862745, 0.58823529, 0.59607843, 0.60784314,\n",
       "       0.61960784, 0.63137255, 0.63921569, 0.64313725, 0.6745098 ,\n",
       "       0.41176471, 0.52118644, 0.65306122, 0.64556962, 0.62331839,\n",
       "       0.63636364, 0.6745098 , 0.6745098 , 0.67843137, 0.67843137,\n",
       "       0.6745098 , 0.67058824, 0.67058824, 0.67058824, 0.45490196,\n",
       "       0.49019608, 0.52156863, 0.54509804, 0.56078431, 0.57254902,\n",
       "       0.59215686, 0.6       , 0.61176471, 0.62352941, 0.63529412,\n",
       "       0.63921569, 0.65490196, 0.65490196, 0.37254902, 0.556     ,\n",
       "       0.65991903, 0.65271967, 0.65560166, 0.66396761, 0.6745098 ,\n",
       "       0.6745098 , 0.67843137, 0.67843137, 0.67843137, 0.6745098 ,\n",
       "       0.6745098 , 0.67058824, 0.45882353, 0.49411765, 0.5254902 ,\n",
       "       0.54901961, 0.56862745, 0.58431373, 0.6       , 0.61176471,\n",
       "       0.61960784, 0.63137255, 0.63921569, 0.64313725, 0.68627451,\n",
       "       0.61176471, 0.34117647, 0.6023622 , 0.6745098 , 0.67330677,\n",
       "       0.66801619, 0.67843137, 0.67843137, 0.67843137, 0.68235294,\n",
       "       0.68235294, 0.68235294, 0.67843137, 0.6745098 , 0.6745098 ,\n",
       "       0.46666667, 0.50196078, 0.53333333, 0.55686275, 0.57254902,\n",
       "       0.58823529, 0.6       , 0.61176471, 0.61133603, 0.63921569,\n",
       "       0.64705882, 0.64313725, 0.72156863, 0.58039216, 0.34901961,\n",
       "       0.64313725, 0.6745098 , 0.68235294, 0.68235294, 0.68235294,\n",
       "       0.67073171, 0.68627451, 0.68627451, 0.68235294, 0.68627451,\n",
       "       0.68235294, 0.67843137, 0.67843137, 0.47843137, 0.50980392,\n",
       "       0.54117647, 0.56078431, 0.57647059, 0.58823529, 0.60392157,\n",
       "       0.61960784, 0.61728395, 0.64285714, 0.65098039, 0.6745098 ,\n",
       "       0.70980392, 0.50196078, 0.36862745, 0.66666667, 0.67843137,\n",
       "       0.68627451, 0.68235294, 0.68627451, 0.69019608, 0.69411765,\n",
       "       0.69411765, 0.69411765, 0.69411765, 0.68627451, 0.68627451,\n",
       "       0.68235294, 0.47843137, 0.51764706, 0.54509804, 0.56862745,\n",
       "       0.58431373, 0.59607843, 0.61176471, 0.62745098, 0.63052209,\n",
       "       0.64705882, 0.65098039, 0.70980392, 0.6745098 , 0.40392157,\n",
       "       0.44313725, 0.68627451, 0.69019608, 0.69803922, 0.69803922,\n",
       "       0.70196078, 0.70196078, 0.70196078, 0.70196078, 0.69803922,\n",
       "       0.70196078, 0.69411765, 0.68627451, 0.68235294, 0.49019608,\n",
       "       0.5254902 , 0.55294118, 0.57647059, 0.58823529, 0.6       ,\n",
       "       0.61568627, 0.63137255, 0.64031621, 0.65490196, 0.65882353,\n",
       "       0.72156863, 0.70196078, 0.45490196, 0.49411765, 0.64705882,\n",
       "       0.69019608, 0.70196078, 0.70588235, 0.70588235, 0.70980392,\n",
       "       0.70588235, 0.70588235, 0.70588235, 0.70196078, 0.69803922,\n",
       "       0.69411765, 0.69019608, 0.50196078, 0.52941176, 0.55686275,\n",
       "       0.58039216, 0.59607843, 0.60392157, 0.61960784, 0.63529412,\n",
       "       0.64705882, 0.65882353, 0.66666667, 0.73333333, 0.70588235,\n",
       "       0.61176471, 0.63137255, 0.48627451, 0.56078431, 0.70196078,\n",
       "       0.69803922, 0.69803922, 0.70980392, 0.71372549, 0.70980392,\n",
       "       0.70588235, 0.70980392, 0.70588235, 0.70196078, 0.70196078,\n",
       "       0.50588235, 0.53333333, 0.56470588, 0.58823529, 0.6       ,\n",
       "       0.60784314, 0.62352941, 0.63921569, 0.65098039, 0.6627451 ,\n",
       "       0.6745098 , 0.73333333, 0.72156863, 0.6       , 0.4       ,\n",
       "       0.45882353, 0.43137255, 0.68627451, 0.6627451 , 0.60392157,\n",
       "       0.71372549, 0.71764706, 0.71764706, 0.71372549, 0.71372549,\n",
       "       0.70980392, 0.70980392, 0.70196078, 0.51372549, 0.54117647,\n",
       "       0.56862745, 0.58823529, 0.60784314, 0.61568627, 0.63137255,\n",
       "       0.64705882, 0.65882353, 0.68235294, 0.74509804, 0.74117647,\n",
       "       0.68627451, 0.57254902, 0.36862745, 0.38039216, 0.44313725,\n",
       "       0.59215686, 0.61960784, 0.50588235, 0.72156863, 0.72156863,\n",
       "       0.72156863, 0.72156863, 0.71764706, 0.71764706, 0.71372549,\n",
       "       0.70588235, 0.51372549, 0.54509804, 0.57254902, 0.59215686,\n",
       "       0.60784314, 0.62352941, 0.63921569, 0.65490196, 0.68627451,\n",
       "       0.71372549, 0.70196078, 0.67058824, 0.62352941, 0.44705882,\n",
       "       0.4       , 0.34901961, 0.4745098 , 0.53333333, 0.53333333,\n",
       "       0.37647059, 0.6745098 , 0.72941176, 0.72941176, 0.7254902 ,\n",
       "       0.7254902 , 0.72156863, 0.71372549, 0.70980392, 0.51372549,\n",
       "       0.54901961, 0.57647059, 0.60392157, 0.61568627, 0.62745098,\n",
       "       0.64313725, 0.7007874 , 0.72941176, 0.74901961, 0.73333333,\n",
       "       0.70588235, 0.61568627, 0.39215686, 0.34509804, 0.32941176,\n",
       "       0.42352941, 0.43529412, 0.49411765, 0.35294118, 0.47058824,\n",
       "       0.72941176, 0.73333333, 0.73333333, 0.72941176, 0.7254902 ,\n",
       "       0.72156863, 0.71372549, 0.52156863, 0.55294118, 0.58431373,\n",
       "       0.60784314, 0.61960784, 0.62745098, 0.68235294, 0.78823529,\n",
       "       0.74117647, 0.64705882, 0.59215686, 0.56078431, 0.57254902,\n",
       "       0.47058824, 0.34117647, 0.30588235, 0.34117647, 0.29803922,\n",
       "       0.42352941, 0.38431373, 0.37647059, 0.70980392, 0.7372549 ,\n",
       "       0.73333333, 0.72941176, 0.72941176, 0.7254902 , 0.71764706,\n",
       "       0.52156863, 0.55294118, 0.58823529, 0.61176471, 0.62745098,\n",
       "       0.63137255, 0.70196078, 0.7689243 , 0.68235294, 0.52941176,\n",
       "       0.38823529, 0.28235294, 0.37254902, 0.5254902 , 0.38039216,\n",
       "       0.28235294, 0.29019608, 0.26666667, 0.45490196, 0.41176471,\n",
       "       0.42352941, 0.73333333, 0.74117647, 0.73333333, 0.73333333,\n",
       "       0.72941176, 0.72941176, 0.7254902 , 0.5254902 , 0.56078431,\n",
       "       0.59215686, 0.61176471, 0.63137255, 0.63921569, 0.70196078,\n",
       "       0.75502008, 0.61176471, 0.43137255, 0.29019608, 0.16470588,\n",
       "       0.20392157, 0.54509804, 0.36862745, 0.2627451 , 0.29411765,\n",
       "       0.29411765, 0.4627451 , 0.41568627, 0.50588235, 0.74117647,\n",
       "       0.74901961, 0.74509804, 0.7372549 , 0.7372549 , 0.73333333,\n",
       "       0.72941176, 0.52941176, 0.56470588, 0.59607843, 0.61960784,\n",
       "       0.63921569, 0.63921569, 0.69411765, 0.75686275, 0.63137255,\n",
       "       0.47843137, 0.32941176, 0.16862745, 0.27843137, 0.5254902 ,\n",
       "       0.31764706, 0.22352941, 0.27843137, 0.34509804, 0.43921569,\n",
       "       0.38431373, 0.61568627, 0.75686275, 0.75686275, 0.75294118,\n",
       "       0.74509804, 0.74509804, 0.74117647, 0.7372549 , 0.53333333,\n",
       "       0.56470588, 0.59607843, 0.61960784, 0.63529412, 0.63921569,\n",
       "       0.69019608, 0.75294118, 0.63888889, 0.50196078, 0.38431373,\n",
       "       0.24313725, 0.23529412, 0.39215686, 0.27843137, 0.29803922,\n",
       "       0.37647059, 0.39607843, 0.41176471, 0.37254902, 0.68235294,\n",
       "       0.76470588, 0.76078431, 0.76078431, 0.76078431, 0.75686275,\n",
       "       0.74901961, 0.74509804, 0.5372549 , 0.56862745, 0.59607843,\n",
       "       0.62352941, 0.64313725, 0.64705882, 0.69803922, 0.74901961,\n",
       "       0.64313725, 0.52941176, 0.44313725, 0.32156863, 0.23137255,\n",
       "       0.34117647, 0.38431373, 0.43529412, 0.47058824, 0.42352941,\n",
       "       0.38039216, 0.42352941, 0.74509804, 0.76862745, 0.76470588,\n",
       "       0.76470588, 0.76078431, 0.75686275, 0.75686275, 0.75294118,\n",
       "       0.54509804, 0.57254902, 0.60392157, 0.62745098, 0.64313725,\n",
       "       0.64705882, 0.68627451, 0.72941176, 0.63921569, 0.54509804,\n",
       "       0.43921569, 0.33333333, 0.2627451 , 0.4       , 0.49411765,\n",
       "       0.52156863, 0.49411765, 0.41176471, 0.40784314, 0.69019608,\n",
       "       0.77254902, 0.77647059, 0.77254902, 0.76862745, 0.76470588,\n",
       "       0.76470588, 0.76078431, 0.75686275, 0.54117647, 0.57647059,\n",
       "       0.60784314, 0.63137255, 0.64705882, 0.65490196, 0.6745098 ,\n",
       "       0.72941176, 0.63921569, 0.5372549 , 0.41960784, 0.34117647,\n",
       "       0.29803922, 0.41568627, 0.47843137, 0.49019608, 0.45882353,\n",
       "       0.37647059, 0.61176471, 0.78039216, 0.78039216, 0.78431373,\n",
       "       0.77647059, 0.76862745, 0.76862745, 0.76470588, 0.76470588,\n",
       "       0.76078431, 0.54509804, 0.58039216, 0.61176471, 0.63921569,\n",
       "       0.65098039, 0.65882353, 0.6745098 , 0.70588235, 0.61960784,\n",
       "       0.51372549, 0.42352941, 0.38823529, 0.3372549 , 0.42352941,\n",
       "       0.4627451 , 0.45490196, 0.40392157, 0.41960784, 0.74901961,\n",
       "       0.79215686, 0.78823529, 0.78431373, 0.78431373, 0.78431373,\n",
       "       0.78039216, 0.77254902, 0.77647059, 0.76862745, 0.54901961,\n",
       "       0.58431373, 0.61568627, 0.64313725, 0.65882353, 0.65490196,\n",
       "       0.69411765, 0.69803922, 0.60784314, 0.51372549, 0.4627451 ,\n",
       "       0.41176471, 0.34117647, 0.39215686, 0.41568627, 0.39215686,\n",
       "       0.37647059, 0.64313725, 0.79215686, 0.79215686, 0.79215686,\n",
       "       0.79215686, 0.79215686, 0.78823529, 0.78431373, 0.78039216,\n",
       "       0.78039216, 0.77647059, 0.54901961, 0.58823529, 0.61568627,\n",
       "       0.64705882, 0.65490196, 0.66666667, 0.70980392, 0.68627451,\n",
       "       0.59607843, 0.50980392, 0.45098039, 0.38431373, 0.32156863,\n",
       "       0.33333333, 0.35294118, 0.38823529, 0.64705882, 0.79215686,\n",
       "       0.79607843, 0.8       , 0.79607843, 0.79607843, 0.79215686,\n",
       "       0.79215686, 0.78823529, 0.78823529, 0.78431373, 0.78431373,\n",
       "       0.55686275, 0.58823529, 0.62352941, 0.64705882, 0.66666667,\n",
       "       0.74901961, 0.67843137, 0.61568627, 0.56470588, 0.46666667,\n",
       "       0.38039216, 0.32941176, 0.30980392, 0.30980392, 0.35686275,\n",
       "       0.6745098 , 0.79215686, 0.79607843, 0.79607843, 0.80392157,\n",
       "       0.8       , 0.8       , 0.8       , 0.79607843, 0.79215686,\n",
       "       0.79215686, 0.78823529, 0.78431373, 0.55686275, 0.59215686,\n",
       "       0.62745098, 0.64705882, 0.7372549 , 0.74509804, 0.73333333,\n",
       "       0.58823529, 0.46666667, 0.42745098, 0.33333333, 0.30980392,\n",
       "       0.30980392, 0.30588235, 0.5372549 , 0.79607843, 0.80392157,\n",
       "       0.80784314, 0.80784314, 0.81176471, 0.81176471, 0.80784314,\n",
       "       0.80784314, 0.8       , 0.80392157, 0.8       , 0.79607843,\n",
       "       0.79215686, 0.55686275, 0.59215686, 0.62745098, 0.6745098 ,\n",
       "       0.76862745, 0.7372549 , 0.7372549 , 0.74509804, 0.52941176,\n",
       "       0.37647059, 0.3372549 , 0.30196078, 0.30196078, 0.30980392,\n",
       "       0.69019608, 0.80392157, 0.81176471, 0.81176471, 0.81176471,\n",
       "       0.81176471, 0.81176471, 0.81176471, 0.80784314, 0.80784314,\n",
       "       0.80784314, 0.8       , 0.79607843, 0.79215686])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "x_train = minmax_scale(x_train)\n",
    "#x_train_normalized.shape\n",
    "\n",
    "print(\"This is how NORMALIZED array looks like:\")\n",
    "x_train[0]  #Prints array example ( first row of csv dataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how a random selected array from dataset, converted to 28*28 image looks like\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_number(image):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#Generates a random integer\n",
    "rnd_idx = np.random.randint(len(y_test))\n",
    "\n",
    "#Reshape array form 784*1 1D np.array   -->   28*28 2D np.array\n",
    "# to make it possible to be plot as greyscale image\n",
    "print(\"This is how a random selected array from dataset, converted to 28*28 image looks like\")\n",
    "plot_number(x_train[rnd_idx].reshape(28,28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones para nuestro modelo\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    '''\n",
    "    x  #muestras, 784\n",
    "    y #muestras, 1\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class np_tensor(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 0])\n",
    "b = a.view(np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.np_tensor"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np_tensor([ True,  True])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a is b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Init parameters utilizando Kaiming He\n",
    "        '''\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
    "    def __call__(self, X): # esta el foward de la clase lineal\n",
    "        Z = self.W @ X + self.b\n",
    "        return Z\n",
    "    def backward(self, X, Z):\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        self.W.grad = Z.grad @ X.T\n",
    "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def backward(self, Z, A):\n",
    "        Z.grad = A.grad.copy()\n",
    "        Z.grad[Z <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_layers():\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        layers - lista que contiene objetos de tipo Linear, ReLU\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.x = None\n",
    "        self.outputs = {}\n",
    "    def __call__(self, X):\n",
    "        self.x = X \n",
    "        self.outputs['l0'] = self.x\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            self.x = layer(self.x)\n",
    "            self.outputs['l'+str(i)]=self.x\n",
    "        return self.x\n",
    "    def backward(self):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])\n",
    "    def update(self, learning_rate = 1e-3):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLU): continue\n",
    "            layer.W = layer.W - learning_rate * layer.W.grad\n",
    "            layer.b = layer.b - learning_rate * layer.b.grad\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.__call__(X))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    batch_size = x.shape[1]\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
    "    preds = probs.copy()\n",
    "    # Costo\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    # Calcular gradientes\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1 #dl/dx\n",
    "    x.grad = probs.copy()\n",
    "    \n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size=128, learning_rate = 1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            scores = model(x.T.view(np_tensor))\n",
    "            _, cost = softmaxXEntropy(scores, y)\n",
    "            model.backward()\n",
    "            model.update(learning_rate)\n",
    "        print(f'costo: {cost}, accuracy: {accuracy(x_val, y_val, mb_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y)):\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
    "        total += pred.shape[1]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to change the output classes according to possible predictions (in this case 24 possible classes found on train_df)\n",
    "\n",
    "#1st hidden layer\n",
    "#Linear(784, 200) --> 784 inputs / 200 outputs\n",
    "#ReLU() Activation type\n",
    "\n",
    "#2nd hidden layer --> 200 inputs / 200 outputs. The inputs are the outputs of the first hidden layer\n",
    "#Linear(200, 200)\n",
    "#ReLU() Activation type\n",
    "\n",
    "#Classification layer  --> 200 inputs / 24 classes to predict\n",
    "#Linear(200,24)\n",
    "\n",
    "model = Sequential_layers([Linear(784, 32), ReLU(), \n",
    "                            Linear(32, 64), ReLU(), \n",
    "                            Linear(64, 128), ReLU(), \n",
    "                            Linear(128,24)])\n",
    "\n",
    "#mini batch size is mainly limited by the hardware we use for the training (OOM error will ocurr when exceeding)\n",
    "# The smaller the batch size, the slower it takes to train\n",
    "mb_size = 2048\n",
    "\n",
    "# The smaller the learning rate is, the slower it takes to train\n",
    "learning_rate = 0.00001\n",
    "\n",
    "#500 epochs were needed to reach a decent level of accuracy using this fully connected network\n",
    "epochs = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo: 3.1575889470844802, accuracy: 0.05298804780876494\n",
      "costo: 3.1233547290148826, accuracy: 0.05239043824701195\n",
      "costo: 3.1173788779020244, accuracy: 0.05517928286852589\n",
      "costo: 3.069979198073029, accuracy: 0.08207171314741035\n",
      "costo: 3.0224684530530848, accuracy: 0.09581673306772909\n",
      "costo: 3.0005973249681936, accuracy: 0.10119521912350597\n",
      "costo: 2.9453296853686095, accuracy: 0.1394422310756972\n",
      "costo: 2.896626449565216, accuracy: 0.14243027888446216\n",
      "costo: 2.857504408519068, accuracy: 0.1747011952191235\n",
      "costo: 2.803683300187088, accuracy: 0.21374501992031872\n",
      "costo: 2.80270667371688, accuracy: 0.1906374501992032\n",
      "costo: 2.695009338952455, accuracy: 0.2\n",
      "costo: 2.648986352297778, accuracy: 0.2239043824701195\n",
      "costo: 2.6677845764402544, accuracy: 0.2647410358565737\n",
      "costo: 2.5839328485934017, accuracy: 0.2647410358565737\n",
      "costo: 2.467830239122109, accuracy: 0.2848605577689243\n",
      "costo: 2.5636599359217436, accuracy: 0.29262948207171313\n",
      "costo: 2.5072101443887305, accuracy: 0.2894422310756972\n",
      "costo: 2.5870321719445775, accuracy: 0.28247011952191237\n",
      "costo: 2.355170888843202, accuracy: 0.29780876494023906\n",
      "costo: 2.215584914354208, accuracy: 0.3318725099601594\n",
      "costo: 2.664301808750693, accuracy: 0.29800796812749003\n",
      "costo: 2.2620454283803735, accuracy: 0.3462151394422311\n",
      "costo: 2.3358365018225253, accuracy: 0.3368525896414343\n",
      "costo: 2.4640339793588133, accuracy: 0.3697211155378486\n",
      "costo: 2.0611323191809134, accuracy: 0.3834661354581673\n",
      "costo: 2.097235625933319, accuracy: 0.39681274900398406\n",
      "costo: 2.1045813693214797, accuracy: 0.4059760956175299\n",
      "costo: 1.9796168024990262, accuracy: 0.43326693227091634\n",
      "costo: 2.660180618480491, accuracy: 0.34601593625498006\n",
      "costo: 2.01840677128413, accuracy: 0.3952191235059761\n",
      "costo: 1.9889427224485703, accuracy: 0.45577689243027886\n",
      "costo: 1.9475976312352057, accuracy: 0.4145418326693227\n",
      "costo: 2.03935540804651, accuracy: 0.46215139442231074\n",
      "costo: 1.873448838077343, accuracy: 0.4649402390438247\n",
      "costo: 2.200384493872683, accuracy: 0.44043824701195217\n",
      "costo: 1.9611923912689009, accuracy: 0.4452191235059761\n",
      "costo: 1.8971174566470177, accuracy: 0.4810756972111554\n",
      "costo: 1.896313172912985, accuracy: 0.449601593625498\n",
      "costo: 1.9954480055636175, accuracy: 0.4685258964143426\n",
      "costo: 1.809022083133941, accuracy: 0.4685258964143426\n",
      "costo: 1.8709241537295875, accuracy: 0.47689243027888445\n",
      "costo: 1.7449677849750909, accuracy: 0.5037848605577689\n",
      "costo: 1.6889583415093152, accuracy: 0.5141434262948207\n",
      "costo: 1.6281058365997634, accuracy: 0.5145418326693227\n",
      "costo: 1.685893646088302, accuracy: 0.5071713147410358\n",
      "costo: 1.5218707658955883, accuracy: 0.5111553784860557\n",
      "costo: 1.653459526446401, accuracy: 0.5205179282868526\n",
      "costo: 1.5683276738593173, accuracy: 0.5021912350597609\n",
      "costo: 1.4887398110209062, accuracy: 0.5270916334661354\n",
      "costo: 1.9147964880353827, accuracy: 0.5089641434262948\n",
      "costo: 1.8303819988770285, accuracy: 0.5286852589641434\n",
      "costo: 1.618878313519484, accuracy: 0.5243027888446216\n",
      "costo: 1.540475993365011, accuracy: 0.5396414342629482\n",
      "costo: 1.4160365636315109, accuracy: 0.5270916334661354\n",
      "costo: 1.5151323652548174, accuracy: 0.5380478087649403\n",
      "costo: 1.6941994502959175, accuracy: 0.48446215139442234\n",
      "costo: 1.8642387619856633, accuracy: 0.5221115537848605\n",
      "costo: 1.4397766647385675, accuracy: 0.5296812749003984\n",
      "costo: 1.4854345808301868, accuracy: 0.5320717131474103\n",
      "costo: 1.7646379812857313, accuracy: 0.4902390438247012\n",
      "costo: 1.3942533315565329, accuracy: 0.5480079681274901\n",
      "costo: 1.4425339772937602, accuracy: 0.5348605577689243\n",
      "costo: 1.5723013658107023, accuracy: 0.552191235059761\n",
      "costo: 1.4232412145876812, accuracy: 0.546812749003984\n",
      "costo: 1.5027786077837504, accuracy: 0.5308764940239044\n",
      "costo: 1.7609087418627, accuracy: 0.5256972111553785\n",
      "costo: 1.32563032259832, accuracy: 0.55199203187251\n",
      "costo: 1.3850207419274962, accuracy: 0.5607569721115537\n",
      "costo: 1.260149151581597, accuracy: 0.5689243027888446\n",
      "costo: 1.6221341333875992, accuracy: 0.5326693227091633\n",
      "costo: 1.4932138981507388, accuracy: 0.5549800796812749\n",
      "costo: 1.2666394890115587, accuracy: 0.5649402390438247\n",
      "costo: 1.9191654833500758, accuracy: 0.550398406374502\n",
      "costo: 1.3599209786119906, accuracy: 0.5657370517928287\n",
      "costo: 1.2658166210001778, accuracy: 0.5621513944223108\n",
      "costo: 1.6286267855201317, accuracy: 0.5354581673306773\n",
      "costo: 1.2679939091888772, accuracy: 0.5705179282868525\n",
      "costo: 1.3522692428302183, accuracy: 0.5655378486055777\n",
      "costo: 1.2984873110081265, accuracy: 0.5711155378486056\n",
      "costo: 1.2691666923286082, accuracy: 0.5537848605577689\n",
      "costo: 1.3472117325735185, accuracy: 0.5591633466135458\n",
      "costo: 1.1093182202440375, accuracy: 0.5840637450199203\n",
      "costo: 1.382445054229866, accuracy: 0.5844621513944223\n",
      "costo: 1.438373666741411, accuracy: 0.5665338645418326\n",
      "costo: 1.1245624529600073, accuracy: 0.5764940239043824\n",
      "costo: 1.543309364426767, accuracy: 0.5573705179282868\n",
      "costo: 1.0779510327655606, accuracy: 0.5870517928286852\n",
      "costo: 1.5152862663650744, accuracy: 0.5707171314741036\n",
      "costo: 0.9824568237372933, accuracy: 0.601792828685259\n",
      "costo: 1.2096143377449318, accuracy: 0.6031872509960159\n",
      "costo: 1.294552105568862, accuracy: 0.5884462151394423\n",
      "costo: 1.1666076352712127, accuracy: 0.6095617529880478\n",
      "costo: 1.1983165974108037, accuracy: 0.5631474103585657\n",
      "costo: 1.549095714875589, accuracy: 0.5856573705179283\n",
      "costo: 1.08536159211205, accuracy: 0.6047808764940239\n",
      "costo: 0.9627266666395292, accuracy: 0.5822709163346613\n",
      "costo: 1.0961853783224136, accuracy: 0.6055776892430279\n",
      "costo: 1.0310016576785908, accuracy: 0.6045816733067729\n",
      "costo: 0.9990270087483692, accuracy: 0.6093625498007968\n",
      "costo: 1.1756961653217042, accuracy: 0.6039840637450199\n",
      "costo: 1.0249285002121602, accuracy: 0.6117529880478088\n",
      "costo: 1.0769574586592952, accuracy: 0.597011952191235\n",
      "costo: 1.3106340719933025, accuracy: 0.5854581673306772\n",
      "costo: 1.1242604793582713, accuracy: 0.6089641434262948\n",
      "costo: 1.242142681850016, accuracy: 0.600199203187251\n",
      "costo: 0.8786646519740877, accuracy: 0.6191235059760957\n",
      "costo: 1.1203128407409595, accuracy: 0.6125498007968128\n",
      "costo: 1.1240847130860903, accuracy: 0.5896414342629482\n",
      "costo: 1.4113510037596462, accuracy: 0.6055776892430279\n",
      "costo: 0.9656343770777955, accuracy: 0.5966135458167331\n",
      "costo: 0.9994423955197883, accuracy: 0.6272908366533865\n",
      "costo: 0.9159816324542587, accuracy: 0.6113545816733068\n",
      "costo: 0.984589206620967, accuracy: 0.6195219123505976\n",
      "costo: 0.8531563936496578, accuracy: 0.6151394422310758\n",
      "costo: 0.8771969543309049, accuracy: 0.6225099601593626\n",
      "costo: 1.140747847768561, accuracy: 0.6061752988047808\n",
      "costo: 0.9378893379158962, accuracy: 0.6326693227091633\n",
      "costo: 1.4473277829738385, accuracy: 0.6262948207171315\n",
      "costo: 1.1042434414526126, accuracy: 0.6264940239043825\n",
      "costo: 0.772970840398603, accuracy: 0.6199203187250996\n",
      "costo: 0.7188691996305001, accuracy: 0.6203187250996016\n",
      "costo: 1.4298971048626703, accuracy: 0.6270916334661355\n",
      "costo: 0.8918778089082567, accuracy: 0.6290836653386455\n",
      "costo: 0.876538358276654, accuracy: 0.6304780876494024\n",
      "costo: 0.9026943248314424, accuracy: 0.6290836653386455\n",
      "costo: 0.7674745938214762, accuracy: 0.6376494023904382\n",
      "costo: 0.9766196003874811, accuracy: 0.6079681274900398\n",
      "costo: 0.8014071484318023, accuracy: 0.647410358565737\n",
      "costo: 1.3298336156758073, accuracy: 0.6249003984063745\n",
      "costo: 0.7528873658750024, accuracy: 0.646613545816733\n",
      "costo: 0.9554842340718186, accuracy: 0.6199203187250996\n",
      "costo: 1.1967797286114317, accuracy: 0.5974103585657371\n",
      "costo: 0.7053824931163325, accuracy: 0.646414342629482\n",
      "costo: 1.1553749032600598, accuracy: 0.6368525896414342\n",
      "costo: 1.0159868825917722, accuracy: 0.6707171314741036\n",
      "costo: 1.0118993930147722, accuracy: 0.6215139442231076\n",
      "costo: 0.8104368559514395, accuracy: 0.647609561752988\n",
      "costo: 0.780637813739321, accuracy: 0.6386454183266932\n",
      "costo: 0.8882421654498674, accuracy: 0.648406374501992\n",
      "costo: 0.8735048410509706, accuracy: 0.6185258964143426\n",
      "costo: 1.7311186373786362, accuracy: 0.6029880478087649\n",
      "costo: 0.817659486653114, accuracy: 0.6249003984063745\n",
      "costo: 0.7623473707381527, accuracy: 0.6649402390438247\n",
      "costo: 0.6832975065242965, accuracy: 0.647609561752988\n",
      "costo: 0.964354522640996, accuracy: 0.6286852589641434\n",
      "costo: 1.3711906939773066, accuracy: 0.595219123505976\n",
      "costo: 1.3007192615878567, accuracy: 0.6472111553784861\n",
      "costo: 0.730111361250187, accuracy: 0.6553784860557769\n",
      "costo: 0.6364163356197834, accuracy: 0.6711155378486056\n",
      "costo: 0.87063613834699, accuracy: 0.6587649402390439\n",
      "costo: 0.7669450146940376, accuracy: 0.6643426294820717\n",
      "costo: 1.1760301502697963, accuracy: 0.6545816733067729\n",
      "costo: 0.6198823771875404, accuracy: 0.6685258964143427\n",
      "costo: 0.7929050859298047, accuracy: 0.6243027888446215\n",
      "costo: 0.7711864506863142, accuracy: 0.6561752988047809\n",
      "costo: 0.7171019712866282, accuracy: 0.6721115537848605\n",
      "costo: 1.5601583302051778, accuracy: 0.6719123505976096\n",
      "costo: 0.7090890034658026, accuracy: 0.6583665338645418\n",
      "costo: 0.8676590529785405, accuracy: 0.6615537848605577\n",
      "costo: 0.8703315813196141, accuracy: 0.6324701195219123\n",
      "costo: 1.1323061781865653, accuracy: 0.6657370517928287\n",
      "costo: 0.9539373176200305, accuracy: 0.6280876494023905\n",
      "costo: 0.7920723596069736, accuracy: 0.6731075697211155\n",
      "costo: 0.6138458471481661, accuracy: 0.6709163346613546\n",
      "costo: 0.5461606313589469, accuracy: 0.6749003984063745\n",
      "costo: 0.6793382397433765, accuracy: 0.6703187250996016\n",
      "costo: 0.6449311360063412, accuracy: 0.6663346613545816\n",
      "costo: 0.7995859034063413, accuracy: 0.6749003984063745\n",
      "costo: 0.8672027217496356, accuracy: 0.6725099601593626\n",
      "costo: 1.9267446055397415, accuracy: 0.600199203187251\n",
      "costo: 0.5135837150571261, accuracy: 0.6788844621513944\n",
      "costo: 0.5181661078516558, accuracy: 0.6804780876494024\n",
      "costo: 0.7616624431034518, accuracy: 0.6792828685258964\n",
      "costo: 0.5875219694597483, accuracy: 0.6780876494023904\n",
      "costo: 0.722184433800603, accuracy: 0.6697211155378486\n",
      "costo: 1.1408564370174394, accuracy: 0.654780876494024\n",
      "costo: 0.681409094327001, accuracy: 0.6717131474103586\n",
      "costo: 0.5305857472295967, accuracy: 0.6950199203187251\n",
      "costo: 1.3248303545533406, accuracy: 0.6458167330677291\n",
      "costo: 0.6100225527116767, accuracy: 0.6898406374501992\n",
      "costo: 0.7428669806310423, accuracy: 0.6846613545816733\n",
      "costo: 0.7980888309102884, accuracy: 0.6880478087649402\n",
      "costo: 0.7293597983125736, accuracy: 0.6800796812749004\n",
      "costo: 0.5779386989566391, accuracy: 0.698406374501992\n",
      "costo: 0.9101611129526003, accuracy: 0.6930278884462151\n",
      "costo: 0.8717553648221287, accuracy: 0.6687250996015937\n",
      "costo: 0.9600622669368333, accuracy: 0.6749003984063745\n",
      "costo: 0.6399269277981207, accuracy: 0.697011952191235\n",
      "costo: 0.5750385822485411, accuracy: 0.6878486055776892\n",
      "costo: 0.923029224489623, accuracy: 0.6832669322709163\n",
      "costo: 0.5992318987628362, accuracy: 0.6918326693227091\n",
      "costo: 1.5171138894944447, accuracy: 0.6227091633466135\n",
      "costo: 0.7582843864634278, accuracy: 0.6747011952191235\n",
      "costo: 0.5238596552360949, accuracy: 0.6928286852589641\n",
      "costo: 0.5102646290931634, accuracy: 0.699402390438247\n",
      "costo: 0.5008615931606113, accuracy: 0.696812749003984\n",
      "costo: 0.8336153101661739, accuracy: 0.6557768924302789\n",
      "costo: 3.2034038076843756, accuracy: 0.5741035856573705\n",
      "costo: 1.036773963093886, accuracy: 0.6611553784860558\n",
      "costo: 0.6967233844457202, accuracy: 0.69601593625498\n",
      "costo: 0.4973970696645676, accuracy: 0.6842629482071713\n",
      "costo: 0.42385656288620144, accuracy: 0.703187250996016\n",
      "costo: 0.44140656321428146, accuracy: 0.7\n",
      "costo: 0.41258162301656703, accuracy: 0.6926294820717132\n",
      "costo: 0.8251740600284254, accuracy: 0.6852589641434262\n",
      "costo: 0.6947892460298893, accuracy: 0.7019920318725099\n",
      "costo: 0.6101689828695743, accuracy: 0.6926294820717132\n",
      "costo: 0.5182139194166727, accuracy: 0.7067729083665338\n",
      "costo: 0.5194012826262057, accuracy: 0.6936254980079681\n",
      "costo: 0.4621506742287423, accuracy: 0.7125498007968127\n",
      "costo: 0.6459893538501571, accuracy: 0.7101593625498008\n",
      "costo: 0.5913960408998001, accuracy: 0.6884462151394423\n",
      "costo: 0.5914599080965404, accuracy: 0.7051792828685259\n",
      "costo: 0.5948482393554222, accuracy: 0.6878486055776892\n",
      "costo: 1.0061993704938708, accuracy: 0.6776892430278885\n",
      "costo: 0.7280900937238657, accuracy: 0.6926294820717132\n",
      "costo: 0.6824754492983488, accuracy: 0.7049800796812749\n",
      "costo: 0.4813607335748531, accuracy: 0.7147410358565737\n",
      "costo: 0.6968698489680389, accuracy: 0.7075697211155378\n",
      "costo: 0.6215784496732931, accuracy: 0.7077689243027888\n",
      "costo: 0.31457119846318937, accuracy: 0.699003984063745\n",
      "costo: 0.5612479969369727, accuracy: 0.7101593625498008\n",
      "costo: 0.8709098667094182, accuracy: 0.6209163346613545\n",
      "costo: 1.2857160618200303, accuracy: 0.6233067729083666\n",
      "costo: 4.705979951634032, accuracy: 0.4207171314741036\n",
      "costo: 0.4217691368602351, accuracy: 0.6844621513944223\n",
      "costo: 0.30572209550590923, accuracy: 0.7059760956175298\n",
      "costo: 0.36587912141039897, accuracy: 0.7141434262948207\n",
      "costo: 0.30543413979026324, accuracy: 0.7081673306772909\n",
      "costo: 0.48911266300300926, accuracy: 0.7159362549800797\n",
      "costo: 0.5462852048572701, accuracy: 0.7119521912350597\n",
      "costo: 0.4456636916654697, accuracy: 0.6946215139442231\n",
      "costo: 0.8133098424928218, accuracy: 0.7009960159362549\n",
      "costo: 0.4337536931389047, accuracy: 0.7129482071713148\n",
      "costo: 0.3792700322978397, accuracy: 0.7149402390438248\n",
      "costo: 0.8419682285077149, accuracy: 0.6619521912350598\n",
      "costo: 1.5610431180382067, accuracy: 0.5675298804780876\n",
      "costo: 0.32033408466241525, accuracy: 0.7097609561752988\n",
      "costo: 0.3386965895877243, accuracy: 0.7017928286852589\n",
      "costo: 0.4239362187072584, accuracy: 0.7085657370517928\n",
      "costo: 0.44627414758750866, accuracy: 0.7115537848605578\n",
      "costo: 0.28775554161214106, accuracy: 0.7165338645418327\n",
      "costo: 0.2585990932517319, accuracy: 0.7093625498007968\n",
      "costo: 0.3952884875354111, accuracy: 0.7302788844621514\n",
      "costo: 0.3615701780515848, accuracy: 0.7131474103585658\n",
      "costo: 0.36982657833465027, accuracy: 0.7163346613545817\n",
      "costo: 0.26679256580020483, accuracy: 0.7099601593625497\n",
      "costo: 0.8753311279629434, accuracy: 0.6840637450199203\n",
      "costo: 0.4599540339701392, accuracy: 0.7121513944223108\n",
      "costo: 0.3568058623591085, accuracy: 0.7197211155378486\n",
      "costo: 0.24882053813205363, accuracy: 0.7185258964143426\n",
      "costo: 0.3772898077804861, accuracy: 0.7225099601593625\n",
      "costo: 2.5531320508812154, accuracy: 0.599800796812749\n",
      "costo: 0.42318383102363477, accuracy: 0.7137450199203187\n",
      "costo: 0.2803516014746373, accuracy: 0.7209163346613546\n",
      "costo: 0.46061833231385924, accuracy: 0.7077689243027888\n",
      "costo: 0.47463948718860477, accuracy: 0.7097609561752988\n",
      "costo: 0.3735328096546278, accuracy: 0.7189243027888447\n",
      "costo: 0.39616776761946043, accuracy: 0.7107569721115538\n",
      "costo: 0.3230316351735881, accuracy: 0.7272908366533865\n",
      "costo: 0.3700927616133059, accuracy: 0.7119521912350597\n",
      "costo: 0.39097373825835396, accuracy: 0.7153386454183267\n",
      "costo: 1.1018882812916249, accuracy: 0.6553784860557769\n",
      "costo: 0.2759827368711886, accuracy: 0.7219123505976096\n",
      "costo: 0.3039109400799035, accuracy: 0.7268924302788845\n",
      "costo: 1.1338626823976956, accuracy: 0.6505976095617529\n",
      "costo: 0.8750192446391915, accuracy: 0.6894422310756972\n",
      "costo: 0.4940102539494195, accuracy: 0.7247011952191235\n",
      "costo: 0.3039048953158994, accuracy: 0.7219123505976096\n",
      "costo: 0.22713697120053297, accuracy: 0.7308764940239044\n",
      "costo: 0.3883601331937837, accuracy: 0.7270916334661355\n",
      "costo: 0.31434875303868, accuracy: 0.7175298804780876\n",
      "costo: 0.26064567927667415, accuracy: 0.7260956175298805\n",
      "costo: 0.23888436019769393, accuracy: 0.7153386454183267\n",
      "costo: 0.3338434552396722, accuracy: 0.7107569721115538\n",
      "costo: 0.28367166447316045, accuracy: 0.7262948207171315\n",
      "costo: 0.42477736023124035, accuracy: 0.7093625498007968\n",
      "costo: 0.2707770903717097, accuracy: 0.7237051792828685\n",
      "costo: 0.27021862298203764, accuracy: 0.7177290836653386\n",
      "costo: 0.5784167602718455, accuracy: 0.7175298804780876\n",
      "costo: 0.27445472360250506, accuracy: 0.7229083665338646\n",
      "costo: 1.2539413343340469, accuracy: 0.6956175298804781\n",
      "costo: 0.2309997902197496, accuracy: 0.7270916334661355\n",
      "costo: 0.43083250728476585, accuracy: 0.7296812749003984\n",
      "costo: 0.2650703789022024, accuracy: 0.7260956175298805\n",
      "costo: 0.19917335856606813, accuracy: 0.7151394422310757\n",
      "costo: 0.191435418297931, accuracy: 0.7211155378486056\n",
      "costo: 1.7903869534200663, accuracy: 0.650796812749004\n",
      "costo: 1.7378262010927534, accuracy: 0.597211155378486\n",
      "costo: 0.5646682171909064, accuracy: 0.7163346613545817\n",
      "costo: 0.222819254427505, accuracy: 0.7366533864541832\n",
      "costo: 0.2324083724394958, accuracy: 0.7338645418326694\n",
      "costo: 0.18871431911321138, accuracy: 0.7292828685258964\n",
      "costo: 0.2214973959063194, accuracy: 0.7384462151394422\n",
      "costo: 0.18563112328466455, accuracy: 0.7304780876494024\n",
      "costo: 0.23625950703821105, accuracy: 0.7318725099601594\n",
      "costo: 0.24651455417571916, accuracy: 0.7270916334661355\n",
      "costo: 0.1843551808781466, accuracy: 0.7334661354581673\n",
      "costo: 0.27911123886515465, accuracy: 0.7308764940239044\n",
      "costo: 1.922955158994268, accuracy: 0.5719123505976096\n",
      "costo: 0.26285246663739936, accuracy: 0.7249003984063745\n",
      "costo: 0.20250484843051425, accuracy: 0.7326693227091633\n",
      "costo: 0.17492777610762267, accuracy: 0.7384462151394422\n",
      "costo: 0.1794218374384454, accuracy: 0.7370517928286853\n",
      "costo: 0.423164111744079, accuracy: 0.7276892430278884\n",
      "costo: 0.2535993732627307, accuracy: 0.7400398406374502\n",
      "costo: 0.26119599247271474, accuracy: 0.7364541832669322\n",
      "costo: 0.21469086779410862, accuracy: 0.7304780876494024\n",
      "costo: 0.23208026025263745, accuracy: 0.7274900398406374\n",
      "costo: 0.2410682929227824, accuracy: 0.7221115537848606\n",
      "costo: 0.36339534581496424, accuracy: 0.7402390438247012\n",
      "costo: 0.30319134225425426, accuracy: 0.7205179282868526\n",
      "costo: 0.1967415909394879, accuracy: 0.7203187250996016\n",
      "costo: 0.20857076473698857, accuracy: 0.7249003984063745\n",
      "costo: 0.18099619849061982, accuracy: 0.7268924302788845\n",
      "costo: 0.1646793733091777, accuracy: 0.7260956175298805\n",
      "costo: 0.31455864802642985, accuracy: 0.7237051792828685\n",
      "costo: 0.16753838440431806, accuracy: 0.7260956175298805\n",
      "costo: 0.17750313636436052, accuracy: 0.7336653386454183\n",
      "costo: 0.15662911975176444, accuracy: 0.7334661354581673\n",
      "costo: 0.5722175769368949, accuracy: 0.7249003984063745\n",
      "costo: 0.8019008782820729, accuracy: 0.7087649402390438\n",
      "costo: 0.7172772924676312, accuracy: 0.7181274900398407\n",
      "costo: 0.2022417853758196, accuracy: 0.7404382470119522\n",
      "costo: 0.12858245479223573, accuracy: 0.7294820717131474\n",
      "costo: 0.14138992157804484, accuracy: 0.7262948207171315\n",
      "costo: 0.6915281930755, accuracy: 0.7294820717131474\n",
      "costo: 1.2587738926969543, accuracy: 0.6597609561752988\n",
      "costo: 4.4249621061493905, accuracy: 0.3533864541832669\n",
      "costo: 0.507184876348341, accuracy: 0.6697211155378486\n",
      "costo: 0.2261826454266795, accuracy: 0.7225099601593625\n",
      "costo: 0.1622815942312586, accuracy: 0.7386454183266933\n",
      "costo: 0.14575542080540244, accuracy: 0.7348605577689243\n",
      "costo: 0.16035189406870104, accuracy: 0.7404382470119522\n",
      "costo: 0.16318098499968725, accuracy: 0.7412350597609562\n",
      "costo: 0.16677923705666425, accuracy: 0.750597609561753\n",
      "costo: 0.12357232727416567, accuracy: 0.7402390438247012\n",
      "costo: 0.1273954867161475, accuracy: 0.7386454183266933\n",
      "costo: 0.15823203771614688, accuracy: 0.7330677290836654\n",
      "costo: 0.1334757381903765, accuracy: 0.7390438247011952\n",
      "costo: 0.12823101880328572, accuracy: 0.7396414342629483\n",
      "costo: 0.14285121985170562, accuracy: 0.7370517928286853\n",
      "costo: 0.16116178439113296, accuracy: 0.7368525896414343\n",
      "costo: 0.14647899297739814, accuracy: 0.7372509960159362\n",
      "costo: 0.17640815619032518, accuracy: 0.7340637450199203\n",
      "costo: 0.19767371406289613, accuracy: 0.7342629482071713\n",
      "costo: 0.11053873053559554, accuracy: 0.7402390438247012\n",
      "costo: 0.12527884622706148, accuracy: 0.7408366533864542\n",
      "costo: 0.17185214049472292, accuracy: 0.7346613545816733\n",
      "costo: 0.14603751068854554, accuracy: 0.7328685258964144\n",
      "costo: 0.1347546401008858, accuracy: 0.7344621513944223\n",
      "costo: 0.41789821792679627, accuracy: 0.7300796812749004\n",
      "costo: 0.11517884850059729, accuracy: 0.7344621513944223\n",
      "costo: 0.09592004030049783, accuracy: 0.7372509960159362\n",
      "costo: 0.11256758076089021, accuracy: 0.7358565737051793\n",
      "costo: 0.6194424304182119, accuracy: 0.7272908366533865\n",
      "costo: 3.444722340598126, accuracy: 0.4209163346613546\n",
      "costo: 0.5723857289629076, accuracy: 0.6890438247011952\n",
      "costo: 0.29686217913123997, accuracy: 0.7249003984063745\n",
      "costo: 0.7708061124672759, accuracy: 0.7097609561752988\n",
      "costo: 1.3174993315221266, accuracy: 0.6631474103585657\n",
      "costo: 3.2736210945444695, accuracy: 0.44402390438247014\n",
      "costo: 0.40060772895489394, accuracy: 0.6848605577689243\n",
      "costo: 0.24192764684833376, accuracy: 0.7141434262948207\n",
      "costo: 0.6170287768170186, accuracy: 0.7127490039840637\n",
      "costo: 0.15865398830644573, accuracy: 0.7521912350597609\n",
      "costo: 0.12255487588329524, accuracy: 0.751593625498008\n",
      "costo: 0.11275486123228802, accuracy: 0.7531872509960159\n",
      "costo: 0.10236464350000174, accuracy: 0.749003984063745\n",
      "costo: 0.11759187347747076, accuracy: 0.7557768924302789\n",
      "costo: 0.09616852874014645, accuracy: 0.748207171314741\n",
      "costo: 0.10853531663528147, accuracy: 0.748804780876494\n",
      "costo: 0.1441451715180845, accuracy: 0.7446215139442232\n",
      "costo: 0.11196229094136391, accuracy: 0.748406374501992\n",
      "costo: 0.10590965428440823, accuracy: 0.749402390438247\n",
      "costo: 0.1018913306547538, accuracy: 0.7462151394422311\n",
      "costo: 0.08764475245807751, accuracy: 0.7424302788844621\n",
      "costo: 0.0955946585323837, accuracy: 0.7446215139442232\n",
      "costo: 0.08965489192261147, accuracy: 0.7440239043824701\n",
      "costo: 0.08963096514453692, accuracy: 0.746414342629482\n",
      "costo: 0.08177962056463915, accuracy: 0.7428286852589642\n",
      "costo: 0.08921871407449171, accuracy: 0.7468127490039841\n",
      "costo: 0.08223508121399971, accuracy: 0.7448207171314741\n",
      "costo: 0.09502810857737036, accuracy: 0.7462151394422311\n",
      "costo: 0.08130940319647846, accuracy: 0.749003984063745\n",
      "costo: 0.07423555140599947, accuracy: 0.74800796812749\n",
      "costo: 0.08160532867375547, accuracy: 0.7442231075697211\n",
      "costo: 0.08114451435316368, accuracy: 0.7486055776892431\n",
      "costo: 0.08001530544686744, accuracy: 0.747211155378486\n",
      "costo: 0.08085820176257276, accuracy: 0.7446215139442232\n",
      "costo: 0.07786314405993983, accuracy: 0.751792828685259\n",
      "costo: 0.07137945046566009, accuracy: 0.750199203187251\n",
      "costo: 0.08094464477215456, accuracy: 0.748406374501992\n",
      "costo: 0.07862745938675388, accuracy: 0.7442231075697211\n",
      "costo: 0.08526034155067394, accuracy: 0.748406374501992\n",
      "costo: 0.08226437515318606, accuracy: 0.7418326693227092\n",
      "costo: 0.14990536591580678, accuracy: 0.75\n",
      "costo: 0.11124118193530554, accuracy: 0.7394422310756972\n",
      "costo: 0.10631527900596041, accuracy: 0.752788844621514\n",
      "costo: 0.07866227087132999, accuracy: 0.7440239043824701\n",
      "costo: 0.07352038082221246, accuracy: 0.7468127490039841\n",
      "costo: 0.07494008144055506, accuracy: 0.7456175298804781\n",
      "costo: 0.0749607290881047, accuracy: 0.7446215139442232\n",
      "costo: 0.08053143973248067, accuracy: 0.749003984063745\n",
      "costo: 0.07178102672535235, accuracy: 0.7420318725099602\n",
      "costo: 0.07459646051025147, accuracy: 0.7478087649402391\n",
      "costo: 0.0680981892531239, accuracy: 0.748207171314741\n",
      "costo: 0.0919139602271019, accuracy: 0.7442231075697211\n",
      "costo: 0.06842197138947045, accuracy: 0.747211155378486\n",
      "costo: 0.0698256568305997, accuracy: 0.7432270916334661\n",
      "costo: 0.061893082738524945, accuracy: 0.7468127490039841\n",
      "costo: 0.07645688594335183, accuracy: 0.747410358565737\n",
      "costo: 0.06435565933049361, accuracy: 0.7428286852589642\n",
      "costo: 0.05833545788999731, accuracy: 0.7460159362549801\n",
      "costo: 0.06119954589332363, accuracy: 0.750597609561753\n",
      "costo: 0.06143980487268356, accuracy: 0.7486055776892431\n",
      "costo: 0.0580924446159957, accuracy: 0.7396414342629483\n",
      "costo: 0.06608136386248059, accuracy: 0.747211155378486\n",
      "costo: 0.07109791232029844, accuracy: 0.7434262948207171\n",
      "costo: 0.06323801189713166, accuracy: 0.74800796812749\n",
      "costo: 0.06300642349802624, accuracy: 0.7446215139442232\n",
      "costo: 0.06798615644959524, accuracy: 0.7458167330677291\n",
      "costo: 0.06509448216391989, accuracy: 0.746414342629482\n",
      "costo: 0.06551849953416046, accuracy: 0.7446215139442232\n",
      "costo: 0.0661876975736989, accuracy: 0.7476095617529881\n",
      "costo: 0.06403830671961412, accuracy: 0.7458167330677291\n",
      "costo: 0.05573849263651811, accuracy: 0.7436254980079682\n",
      "costo: 0.06217192945692351, accuracy: 0.7466135458167331\n",
      "costo: 0.061655336564719346, accuracy: 0.7416334661354582\n",
      "costo: 0.05418715767503036, accuracy: 0.7446215139442232\n",
      "costo: 0.05939746724640852, accuracy: 0.7468127490039841\n",
      "costo: 0.05841228738654771, accuracy: 0.746414342629482\n",
      "costo: 0.058605280088254545, accuracy: 0.74800796812749\n",
      "costo: 0.055110714977066266, accuracy: 0.7448207171314741\n",
      "costo: 0.05422878339555269, accuracy: 0.750398406374502\n",
      "costo: 0.048175248964880284, accuracy: 0.7486055776892431\n",
      "costo: 0.05295648655017658, accuracy: 0.748406374501992\n",
      "costo: 0.05279609366670492, accuracy: 0.7448207171314741\n",
      "costo: 0.05472739648600958, accuracy: 0.748804780876494\n",
      "costo: 0.049587149706581284, accuracy: 0.748207171314741\n",
      "costo: 0.05104045034141345, accuracy: 0.7450199203187251\n",
      "costo: 0.05056117073968654, accuracy: 0.7444223107569721\n",
      "costo: 0.05475484632734062, accuracy: 0.7452191235059761\n",
      "costo: 0.05509247979739664, accuracy: 0.7444223107569721\n",
      "costo: 0.05028247653853781, accuracy: 0.74800796812749\n",
      "costo: 0.05716586269518283, accuracy: 0.7418326693227092\n",
      "costo: 0.05422729409231273, accuracy: 0.7410358565737052\n",
      "costo: 0.05544192054873727, accuracy: 0.7434262948207171\n",
      "costo: 0.05581545422637089, accuracy: 0.7450199203187251\n",
      "costo: 0.05127438821899723, accuracy: 0.7428286852589642\n",
      "costo: 0.04740821988012229, accuracy: 0.7466135458167331\n",
      "costo: 0.0463356672085168, accuracy: 0.7448207171314741\n",
      "costo: 0.0549719912545759, accuracy: 0.7450199203187251\n",
      "costo: 0.048811219360772394, accuracy: 0.749203187250996\n",
      "costo: 0.05163984008664423, accuracy: 0.7462151394422311\n",
      "costo: 0.04619774437574945, accuracy: 0.7436254980079682\n",
      "costo: 0.04964801907167539, accuracy: 0.7426294820717132\n",
      "costo: 0.05082098901526314, accuracy: 0.7414342629482071\n",
      "costo: 0.047376892450427664, accuracy: 0.7476095617529881\n",
      "costo: 0.04769634261657451, accuracy: 0.7466135458167331\n",
      "costo: 0.04332980055756832, accuracy: 0.749601593625498\n",
      "costo: 0.04454670695863259, accuracy: 0.745418326693227\n",
      "costo: 0.037057721190688245, accuracy: 0.7468127490039841\n",
      "costo: 0.04512968370105907, accuracy: 0.7462151394422311\n",
      "costo: 0.03796564553203627, accuracy: 0.7468127490039841\n",
      "costo: 0.041380134410065175, accuracy: 0.7436254980079682\n",
      "costo: 0.041545336529367954, accuracy: 0.7428286852589642\n",
      "costo: 0.03959546311408942, accuracy: 0.746414342629482\n",
      "costo: 0.038547583119381265, accuracy: 0.7432270916334661\n",
      "costo: 0.044244257988389296, accuracy: 0.7452191235059761\n",
      "costo: 0.03877847234573007, accuracy: 0.7448207171314741\n",
      "costo: 0.03888274979149761, accuracy: 0.7456175298804781\n",
      "costo: 0.04060441159255662, accuracy: 0.7424302788844621\n",
      "costo: 0.04161112996804007, accuracy: 0.7466135458167331\n",
      "costo: 0.04110776688494057, accuracy: 0.7446215139442232\n",
      "costo: 0.03763955691924251, accuracy: 0.7452191235059761\n",
      "costo: 0.04619887438607148, accuracy: 0.7440239043824701\n",
      "costo: 0.04286548712510917, accuracy: 0.7444223107569721\n",
      "costo: 0.036273369173439936, accuracy: 0.7416334661354582\n",
      "costo: 0.03481369467704374, accuracy: 0.7412350597609562\n",
      "costo: 0.037264105666611244, accuracy: 0.7462151394422311\n",
      "costo: 0.03906544536685174, accuracy: 0.7432270916334661\n",
      "costo: 0.04022673729598026, accuracy: 0.7394422310756972\n",
      "costo: 0.03740483903091003, accuracy: 0.7414342629482071\n",
      "costo: 0.03983374601372937, accuracy: 0.7426294820717132\n",
      "costo: 0.035422385077623776, accuracy: 0.7444223107569721\n",
      "costo: 0.03769624654356926, accuracy: 0.7432270916334661\n",
      "costo: 0.03722943615518796, accuracy: 0.7424302788844621\n",
      "costo: 0.04371976664442264, accuracy: 0.7410358565737052\n",
      "costo: 0.03828472679791853, accuracy: 0.7434262948207171\n",
      "costo: 0.03434467303173676, accuracy: 0.7406374501992032\n",
      "costo: 0.03484722051249646, accuracy: 0.7460159362549801\n",
      "costo: 0.03389433825030774, accuracy: 0.7440239043824701\n",
      "costo: 0.03497672463415801, accuracy: 0.7420318725099602\n",
      "costo: 0.03436994544172274, accuracy: 0.746414342629482\n",
      "costo: 0.0343141468608593, accuracy: 0.7440239043824701\n",
      "costo: 0.03611911562224731, accuracy: 0.7468127490039841\n",
      "costo: 0.034739243561658924, accuracy: 0.7430278884462151\n",
      "costo: 0.032733901216414674, accuracy: 0.7450199203187251\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Accuracy reached\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.72118959107806 %\n"
     ]
    }
   ],
   "source": [
    "print((accuracy(x_test, y_test, mb_size))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor predicho es: d el valor real es:d\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test[idx].reshape(28,28))\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'el valor predicho es: {alphabet[pred]} el valor real es:{alphabet[y_test[idx]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
