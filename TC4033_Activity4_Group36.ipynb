{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Alumnos:\n",
        "*   Andre Nicolai Gutiérrez Bautista\n",
        "*   Fernando Guzmán Briones\n",
        "*   Julio Osvaldo Hernández Bucio\n",
        "*   Genaro Rodríguez Vázquez\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries and requirements\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJo5OT3ndJym"
      },
      "id": "aJo5OT3ndJym"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Don't forget to install the portalocker to use *WikiText2()*. Once that you already finish with it restart the environment and you can ignore the line below by adding a #."
      ],
      "metadata": {
        "id": "vmRtfWsmdsAA"
      },
      "id": "vmRtfWsmdsAA"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install portalocker"
      ],
      "metadata": {
        "id": "-G9SpiEQ9tjc"
      },
      "id": "-G9SpiEQ9tjc",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# Dataloader library\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign the used device. Remember to work with the GPU to work faster."
      ],
      "metadata": {
        "id": "MiHAPYZcd3pC"
      },
      "id": "MiHAPYZcd3pC"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6d8ff971",
      "metadata": {
        "id": "6d8ff971"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Splitting Data"
      ],
      "metadata": {
        "id": "GdVsNxf5dyOf"
      },
      "id": "GdVsNxf5dyOf"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "tokeniser = get_tokenizer('basic_english')\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "# Set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "seq_length = 50\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
        "#     target_data = torch.cat(d)\n",
        "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n",
        "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n",
        "\n",
        "# Create tensors for the training set\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # choose a batch size that fits your computation resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model"
      ],
      "metadata": {
        "id": "zMJbPPtjeLzp"
      },
      "id": "zMJbPPtjeLzp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition"
      ],
      "metadata": {
        "id": "vKZRy_nZeiNi"
      },
      "id": "vKZRy_nZeiNi"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1) # Original 0.25. Experimenting with 0.1\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        embeddings = self.embeddings(text)\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "        decoded = self.fc(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Instantiation"
      ],
      "metadata": {
        "id": "DeDm3K83emTS"
      },
      "id": "DeDm3K83emTS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters"
      ],
      "metadata": {
        "id": "jS5RalBsesyV"
      },
      "id": "jS5RalBsesyV"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 100 # embedding size\n",
        "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 6 # the number of nn.LSTM layers (changed from 2 to 6)"
      ],
      "metadata": {
        "id": "TNQFrsnAeka7"
      },
      "id": "TNQFrsnAeka7",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM instance"
      ],
      "metadata": {
        "id": "w0efjcC4ey4s"
      },
      "id": "w0efjcC4ey4s"
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)"
      ],
      "metadata": {
        "id": "ia4nUwove5GI"
      },
      "id": "ia4nUwove5GI",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "AVTBTWnkfBTH"
      },
      "id": "AVTBTWnkfBTH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method definition"
      ],
      "metadata": {
        "id": "tr7x8Q1gfyDK"
      },
      "id": "tr7x8Q1gfyDK"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "215eabb9",
      "metadata": {
        "id": "215eabb9"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, optimiser):\n",
        "  model = model.to(device=device)\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i, (data, targets) in enumerate((train_loader)):\n",
        "      # Zero the gradients\n",
        "      optimiser.zero_grad()\n",
        "\n",
        "      # Place data in device\n",
        "      data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "      # Initialize hidden states\n",
        "      hidden = model.init_hidden(data.size(0))\n",
        "\n",
        "      # Forward pass\n",
        "      outputs, hidden = model(data, hidden)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = loss_function(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      # Update parameters\n",
        "      optimiser.step()\n",
        "      total_loss += loss.item()\n",
        "    # Print information\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}')\n",
        "  print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters"
      ],
      "metadata": {
        "id": "3Z0O5e1cf0Vh"
      },
      "id": "3Z0O5e1cf0Vh"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aa9c84ce",
      "metadata": {
        "id": "aa9c84ce"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model training"
      ],
      "metadata": {
        "id": "JR4dXEIsf39A"
      },
      "id": "JR4dXEIsf39A"
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, epochs, optimiser)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju7pZUTkgBmw",
        "outputId": "2ca81102-b82f-40a5-9ee6-65cf874b97ec"
      },
      "id": "ju7pZUTkgBmw",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 7.0322\n",
            "Epoch [2/10], Loss: 6.9416\n",
            "Epoch [3/10], Loss: 6.9305\n",
            "Epoch [4/10], Loss: 6.8493\n",
            "Epoch [5/10], Loss: 6.2517\n",
            "Epoch [6/10], Loss: 5.9325\n",
            "Epoch [7/10], Loss: 5.7531\n",
            "Epoch [8/10], Loss: 5.6234\n",
            "Epoch [9/10], Loss: 5.5188\n",
            "Epoch [10/10], Loss: 5.4314\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "jx_1-FS_gIxF"
      },
      "id": "jx_1-FS_gIxF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Text method"
      ],
      "metadata": {
        "id": "isvYIQvjgOTS"
      },
      "id": "isvYIQvjgOTS"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c8667411",
      "metadata": {
        "id": "c8667411"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_text, num_words, temperature=1.0):\n",
        "  # Model into evaluation mode\n",
        "  model.eval()\n",
        "  # Tokenize the start text\n",
        "  words = tokeniser(start_text)\n",
        "  # Start the hidden state. 1 to generate text word after word.\n",
        "  hidden = model.init_hidden(1)\n",
        "\n",
        "  for _ in range(num_words):\n",
        "    # Transform the list of words in tensors by using the vocab.\n",
        "    x = torch.tensor([[vocab[word] for word in words]], dtype=torch.long, device=device)\n",
        "    # Predict the next word.\n",
        "    y_pred, hidden = model(x, hidden)\n",
        "\n",
        "    last_word_logits = y_pred[:, -1, :]\n",
        "    # Obtain the probability distributions\n",
        "    p = (F.softmax(last_word_logits / temperature, dim=-1).detach()).to(device='cpu').numpy()\n",
        "    # Identify the word index by using the most probable next word.\n",
        "    word_index = np.random.choice(len(last_word_logits[0]), p=p[0])\n",
        "    # Add the word to the list of selected words\n",
        "    words.append(vocab.lookup_token(word_index))\n",
        "\n",
        "  return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test to generate text"
      ],
      "metadata": {
        "id": "TP3xiOKNgXVn"
      },
      "id": "TP3xiOKNgXVn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text with 5 words\n",
        "print(generate_text(model, start_text=\"I like \", num_words=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FONqsYTW5sSE",
        "outputId": "a8a77c0c-9919-469d-d735-36040454fd18"
      },
      "id": "FONqsYTW5sSE",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like the specialized machines by history\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text with 5 words and temperature different than 1\n",
        "print(generate_text(model, start_text=\"I like \", num_words=5, temperature = 0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmoJRA4Z6_Hc",
        "outputId": "dca9476b-6994-4279-e585-89660f897cdd"
      },
      "id": "HmoJRA4Z6_Hc",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like the <unk> of the <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text with 20 words and temperature different than 1\n",
        "print(generate_text(model, start_text=\"I like \", num_words=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOA4nOVR7hcl",
        "outputId": "28877515-938d-4b2f-c8cb-118cf85fb40f"
      },
      "id": "tOA4nOVR7hcl",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like three of the best of the commonwealth limits . the largest credited may the central zealand census ( 15 hectares\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text with 20 words and temperature different than 1\n",
        "print(generate_text(model, start_text=\"I like \", num_words=20, temperature=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPZvMnFL56LH",
        "outputId": "e0b63315-c76c-4f48-c1fa-cb80eb8d44d8"
      },
      "id": "tPZvMnFL56LH",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like the first time of the game . he was to gain a <unk> of the song was not known by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text with 100 word\n",
        "print(generate_text(model, start_text=\"I like \", num_words=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjdJKletggRl",
        "outputId": "6efd2db4-39e2-4a51-9ffc-1ba7207508ab"
      },
      "id": "CjdJKletggRl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like the part of the early season of some of the most poems by one signals . after the evidence , shrubs of buildings in the holy clay classics , fu <unk> , in each protestant game in april lone support when it is beaten by the next day the goal . after <unk> , the definition aimed at an one @-@ month strike who debuted from his second game this service , despite dylan perón began which was adequate to keep the landmark who as a seat of to join a realistic donations . for pulaski ' s worth law\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions"
      ],
      "metadata": {
        "id": "4QWaVuL019pX"
      },
      "id": "4QWaVuL019pX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This excercise was shorter than the text classifier one with a couple of differences but also similar ones.\n",
        "For example, about how to start both excersies is quite similar, by doing first a tokenizer to build the vocabulary and obtain the tokens for each word. After that, we split the data into val, test and traing and we have to assign the sets to the dataloader in both of them.\n",
        "\n",
        "Also, into the train method we can find the main similarity, where we have to include the put in zero the gradients, use the backward pass, and optimization.\n",
        "\n",
        "In this case we are using the Cross Entropy Loss, however it is also possible to adapt it to use the NLLLoss like in the classifier excercise.\n",
        "\n",
        "We used a dropout of 0.1 instead of 0.25, used 6 layers instead of 2 and used 10 epochs instead of only 5.\n",
        "\n",
        "We performed the following tests:\n",
        "\n",
        "\n",
        "1.   Creating a short text (5 Words).\n",
        "2.   Creating a short text with temperature different than 1.\n",
        "3.   Creating a not to short text (20 word).\n",
        "4.   Creating a not to short text with temperature different than 1.\n",
        "5.   Creating a text with 100 words.\n",
        "\n",
        "And the results were the following:\n",
        "\n",
        "1.   i like the specialized machines by history\n",
        "2.   i like the </unk/> of the </unk/>\n",
        "3.   i like three of the best of the commonwealth limits . the largest credited may the central zealand census ( 15 hectares\n",
        "4.   i like the first time of the game . he was to gain a </unk/> of the song was not known by\n",
        "5. i like the part of the early season of some of the most poems by one signals . after the evidence , shrubs of buildings in the holy clay classics , fu </unk/> , in each protestant game in april lone support when it is beaten by the next day the goal . after </unk/> , the definition aimed at an one @-@ month strike who debuted from his second game this service , despite dylan perón began which was adequate to keep the landmark who as a seat of to join a realistic donations . for pulaski ' s worth law\n",
        "\n",
        "\n",
        "We noticed that, our model creates more logic sentences when we ask for a text with few words than with a lot of words. The text with 5 words has more sence than the one with 20 words but the 20 words sentence has more sence than the one with 100 words. This is because we are working with the probability of the next word.\n",
        "Also, when we tried to change the temperature we received a lot of \"</unk/>\". So, it is better to ask for just a short sentence with temperature 100.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-fgf2G_b2A-6"
      },
      "id": "-fgf2G_b2A-6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}